<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Victor Boussange" />

  
  
  
    
  
  <meta name="description" content="In this tutorial, you will learn about different techniques to infer parameters of a (differentiable) process-based model against data." />

  
  <link rel="alternate" hreflang="en-us" href="https://vboussange.github.io/post/primer_mechanistic_inference/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.e86434ae70bd28d2a9bb384cdaa99ea3.css" />

  



  


  


  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hua3c898eed18b98379688891d5011a1d0_203625_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hua3c898eed18b98379688891d5011a1d0_203625_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://vboussange.github.io/post/primer_mechanistic_inference/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@wowchemy" />
    <meta property="twitter:creator" content="@wowchemy" />
  
  <meta property="og:site_name" content="Victor Boussange" />
  <meta property="og:url" content="https://vboussange.github.io/post/primer_mechanistic_inference/" />
  <meta property="og:title" content="A primer on mechanistic inference with differentiable process-based models in Julia | Victor Boussange" />
  <meta property="og:description" content="In this tutorial, you will learn about different techniques to infer parameters of a (differentiable) process-based model against data." /><meta property="og:image" content="https://vboussange.github.io/media/icon_hua3c898eed18b98379688891d5011a1d0_203625_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://vboussange.github.io/media/icon_hua3c898eed18b98379688891d5011a1d0_203625_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2025-01-02T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2025-01-02T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://vboussange.github.io/post/primer_mechanistic_inference/"
  },
  "headline": "A primer on mechanistic inference with differentiable process-based models in Julia",
  
  "datePublished": "2025-01-02T00:00:00Z",
  "dateModified": "2025-01-02T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Victor Boussange"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Victor Boussange",
    "logo": {
      "@type": "ImageObject",
      "url": "https://vboussange.github.io/media/icon_hua3c898eed18b98379688891d5011a1d0_203625_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "In this tutorial, you will learn about different techniques to infer parameters of a (differentiable) process-based model against data."
}
</script>

  

  

  

  

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [ ['$','$'], ["\\(","\\)"] ],
processEscapes: true
}
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <title>A primer on mechanistic inference with differentiable process-based models in Julia | Victor Boussange</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="459357d6d6c3ff066c785fbc41554f84" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.e22a2a20712150175b9cd707be2d0584.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Victor Boussange</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Victor Boussange</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#softwares"><span>üßëüèΩ‚Äçüíª Softwares</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>üìÑ Publications & Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#teaching"><span>üéì Teaching & resources</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>üí¨ Posts</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>A primer on mechanistic inference with differentiable process-based models in Julia</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Victor Boussange</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jan 2, 2025
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    27 min read
  </span>
  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Here you will learn about different techniques to infer parameters of a
(differentiable) process-based model against data. This is useful to in
the context of mechanistic inference, where we want to explain patterns
in a system by understanding the processes that generate them, in
contrast to purely statistical or empirical inference, which might
identify patterns or correlations in data without necessarily
understanding the causes. We‚Äôll mostly focus on differential equation
models. Make sure that you stick to the end, where we‚Äôll see how we can
not only infer parameter values but also functional forms, by
parametrizing models‚Äô components with neural networks.</p>
<h1 id="preliminaries">Preliminaries</h1>
<h2 id="wait-what-is-a-differentiable-model">Wait, what is a differentiable model?</h2>
<p>One can usually write a model as a map ‚Ñ≥ mapping some parameters <em>p</em>, an
initial state <em>u</em><sub>0</sub> and a time <em>t</em> to a future state
<em>u</em><sub><em>t</em></sub></p>
<p><em>u</em><sub><em>t</em></sub>‚ÄÑ=‚ÄÑ‚Ñ≥(<em>u</em><sub>0</sub>,‚ÄÜ<em>t</em>,‚ÄÜ<em>p</em>).</p>
<p>We call <strong>differentiable</strong> a model ‚Ñ≥ for which we can calculate its
derivative with respect to <em>p</em> or <em>u</em><sub>0</sub>. The derivative
$\frac{\partial \mathcal{M}}{\partial \theta}$ expresses how much the
model output changes with respect to a small change in <em>Œ∏</em>.</p>
<blockquote>
<p><strong>Recall your Calculus class!</strong></p>
<p>$$\frac{df}{dx}(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$</p>
</blockquote>
<p>Let‚Äôs illustrate this concept with the <a href="https://en.wikipedia.org/wiki/Logistic_function#In_ecology:_modeling_population_growth" target="_blank" rel="noopener">logistic equation
model</a>.
This model has an analytic formulation given by:</p>
<p>$$\mathcal{M}(u_0, p, t) = \frac{K}{1 + \big( \frac{K-u_0}{u_0} \big) e^{rt}}$$</p>
<p>Let‚Äôs code it</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">UnPack</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Plots</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Random</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">ComponentArrays</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">BenchmarkTools</span>
</span></span><span class="line"><span class="cl"><span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">mymodel</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">T</span> <span class="o">=</span> <span class="n">eltype</span><span class="p">(</span><span class="n">u0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@unpack</span> <span class="n">r</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@.</span> <span class="n">K</span> <span class="o">/</span> <span class="p">(</span><span class="n">one</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">K</span> <span class="o">-</span> <span class="n">u0</span><span class="p">)</span> <span class="o">/</span> <span class="n">u0</span> <span class="o">*</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span> <span class="o">*</span> <span class="n">t</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">r</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">u0</span> <span class="o">=</span> <span class="mf">0.005</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tsteps</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">mymodel</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">tsteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">tsteps</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-2-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<blockquote>
<p><strong>What is a <code>ComponentArray</code>?</strong></p>
<p>A <code>ComponentArray</code> is a convenient Array type that allows to access
array elements with symbols, similarly to a <code>NamedTuple</code>, while
behaving like a standard array. For instance, you could do something
like</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">cv</span> <span class="o">=</span> <span class="n">ComponentVector</span><span class="p">(;</span><span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">cv</span> <span class="o">.=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</span></span></code></pre></div><pre><code>ComponentVector{Int64}(a = 3, b = 4)
</code></pre>
<p>This is useful, because you can only calculate a gradient w.r.t a
<code>Vector</code>!</p>
</blockquote>
<p>Now let‚Äôs try to calculate the gradient of this model. While you could
in this case derive the gradient analytically, an analytic derivation is
generally tricky with complex models. And what about models that can
only be simulated numerically, with no analytic expressions? We need to
find a more automatized way to calculate gradients.</p>
<p>How about <a href="https://en.wikipedia.org/wiki/Finite_difference_method" target="_blank" rel="noopener">the finite difference
method</a>?</p>
<blockquote>
<p><strong>Exercise: finite differences</strong></p>
<p>Implement the function <code>‚àÇmymodel_‚àÇK(h, u0, p, t)</code> which returns the
model‚Äôs derivative with respect to <code>K</code>, calculated with a small <code>h</code> to
be provided by the user.</p>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">‚àÇmymodel_‚àÇK</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">phat</span> <span class="o">=</span> <span class="p">(;</span> <span class="n">r</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">K</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">(</span><span class="n">mymodel</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="n">phat</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="o">-</span> <span class="n">mymodel</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span> <span class="o">/</span> <span class="n">h</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="n">‚àÇmymodel_‚àÇK</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</span></span></code></pre></div><p>0.00010443404854589694</p>
</details>
<p>The gradient of the model is useful to understand how a parameter
influences the output of the model. Let‚Äôs calculate the importance of
the carrying capacity <code>K</code> on the model output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">dm_dp</span> <span class="o">=</span> <span class="n">‚àÇmymodel_‚àÇK</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">tsteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">tsteps</span><span class="p">,</span> <span class="n">dm_dp</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-6-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>As you can observe, the carrying capacity has no effect at small <em>t</em>
where population is small, and its influence on the dynamics grows as
the population grows. We expect the reverse effect for <em>r</em>.</p>
<h2 id="on-the-importance-of-gradients-for-inference">On the importance of gradients for inference</h2>
<p>The ability to calculate the derivative of a model is crucial when it
comes to inference. Both within a full Bayesian inference context, where
one wants to sample the posterior distribution of parameters <em>Œ∏</em> given
data <em>u</em>, <em>p</em>(<em>Œ∏</em>|<em>u</em>), or when one wants to obtain a point estimate
$\theta^\star = \text{argmax}_\theta (p(\theta | u))$ (frequentist or machine
learning context), the model gradient proves very useful. In a full
Bayesian inference context, they are used e.g.¬†with Hamiltonian Markov
Chains methods, such as the NUTS sampler, and in a machine learning
context, they are used with gradient-based optimizer.</p>
<h3 id="gradient-descent">Gradient descent</h3>
<p>The best way to grasp the importance of gradients in inference is to
understand the <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">gradient descent
algorithm</a>.</p>
<p>The following picture illustrates the algorithm in the special case
where <em>p</em> is one-dimensional.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>Given an initial estimate of the parameter value <em>p</em><sub>0</sub>,
$\frac{d \mathcal{M}}{dp}$ is used to suggest a new, better estimate,
following</p>
<p>$$p_{n+1} = p_n - \eta \frac{d \mathcal{M}}{dp}(u_0, t, p) $$</p>
<p>where <em>Œ∑</em> is the learning rate.</p>
<p>Gradient-based methods are usually very efficient in high-dimensional
spaces.</p>
<h2 id="automatic-differentiation">Automatic differentiation</h2>
<p>Let‚Äôs go back to our method <code>‚àÇmymodel_‚àÇp</code>. What is the optimal value of
<code>h</code> to calculate the derivative? This is a tricky question, because a
too small <code>h</code> can lead to round off errors (<a href="https://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_%28AD%29_via_High_Dimensional_Algebras/" target="_blank" rel="noopener">see more explanations
here</a>)
while <code>h</code> too large also leads to a bad approximation of the asymptotic
definition.</p>
<!-- Also, can you calculate how many evaluations of the model do you need if your parameter is $d$ dimensionsal?
$\mathcal{O}(2 d)$ -->
<p>Fortunately, a bunch of techniques referred to as <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="noopener"><strong>automatic
differentiation</strong></a>
(AD) allows to <strong>exactly</strong> differentiate any piece of numerical
functions. In practice, your code must be exclusively written within an
AD-backend, such as Torch, JAX or Tensorflow. Those libraries do not
know how to differentiate code not written in their own language, such
as normal Python code.</p>
<p>Fortunately, Julia is an <em>AD-pervasive language</em>! This means that any
piece of Julia code is theoretically differentiable with AD.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">ForwardDiff</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@btime</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">p</span> <span class="o">-&gt;</span> <span class="n">mymodel</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">p</span><span class="p">);</span>
</span></span></code></pre></div><pre><code>  1.225 Œºs (12 allocations: 432 bytes)
</code></pre>
<p>This is what makes Julia great for model calibration and inference!
Write your model in Julia, and any inference method using AD will be
able to work with your model!</p>
<p>To learn more about AD in Julia, check-out this <a href="https://gdalle.github.io/AutodiffTutorial/" target="_blank" rel="noopener">cool
blog-post</a> and <a href="https://gdalle.github.io/JuliaCon2024-AutoDiff/#/title-slide" target="_blank" rel="noopener">this short
presentation</a>.</p>
<p>Now let‚Äôs get started with inference.</p>
<h1 id="mechanistic-inference">Mechanistic inference</h1>
<h2 id="the-mechanistic-model-and-the-data">The mechanistic model and the data</h2>
<p>We‚Äôll use a simple dynamical community model, the <a href="https://en.wikipedia.org/wiki/Lotka%e2%80%93Volterra_equations" target="_blank" rel="noopener">Lotka
Volterra</a> model,
to generate data. We‚Äôll then contaminate this data with noise, and try
to recover the parameters that have generated the data. The goal of the
session will be to estimate those parameters from the data, using a
bunch of different techniques.</p>
<p>So let‚Äôs first generate the data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">OrdinaryDiffEq</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># Define Lotka-Volterra model.</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">lotka_volterra</span><span class="p">(</span><span class="n">du</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Model parameters.</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@unpack</span> <span class="n">Œ±</span><span class="p">,</span> <span class="n">Œ≤</span><span class="p">,</span> <span class="n">Œ≥</span><span class="p">,</span> <span class="n">Œ¥</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Current state.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">u</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Evaluate differential equations.</span>
</span></span><span class="line"><span class="cl">    <span class="n">du</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Œ±</span> <span class="o">-</span> <span class="n">Œ≤</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="c"># prey</span>
</span></span><span class="line"><span class="cl">    <span class="n">du</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Œ¥</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">Œ≥</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span> <span class="c"># predator</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">nothing</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># Define initial-value problem.</span>
</span></span><span class="line"><span class="cl"><span class="n">u0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">p_true</span> <span class="o">=</span> <span class="p">(;</span><span class="n">Œ±</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">Œ≤</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Œ≥</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">Œ¥</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c"># tspan = (hudson_bay_data[1,:t], hudson_bay_data[end,:t])</span>
</span></span><span class="line"><span class="cl"><span class="n">tspan</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tsteps</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="n">tspan</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tspan</span><span class="p">[</span><span class="k">end</span><span class="p">],</span> <span class="mi">51</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">alg</span> <span class="o">=</span> <span class="n">Tsit5</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">prob</span> <span class="o">=</span> <span class="n">ODEProblem</span><span class="p">(</span><span class="n">lotka_volterra</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">tspan</span><span class="p">,</span> <span class="n">p_true</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">saveat</span> <span class="o">=</span> <span class="n">tsteps</span>
</span></span><span class="line"><span class="cl"><span class="n">sol_true</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">alg</span><span class="p">;</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c"># Plot simulation.</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">sol_true</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-8-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>This is the true state of the system. Now let‚Äôs contaminate it with
observational noise.</p>
<blockquote>
<p><strong>Exercise: contaminate data with noise</strong></p>
<p>Create a <code>data_mat</code> array, which consists of the ODE solution
contaminated with a lognormally-distributed noise with standard
deviation <code>0.3</code>.</p>
<blockquote>
<p><strong>Note</strong></p>
<p>Note that we add lognormally-distributed noise instead of
normally-distributed because we are observing population abundance,
which can only be positive.</p>
</blockquote>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">data_mat</span> <span class="o">=</span> <span class="kt">Array</span><span class="p">(</span><span class="n">sol_true</span><span class="p">)</span> <span class="o">.*</span> <span class="n">exp</span><span class="o">.</span><span class="p">(</span><span class="mf">0.3</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">sol_true</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="c"># Plot simulation and noisy observations.</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">sol_true</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">scatter!</span><span class="p">(</span><span class="n">sol_true</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="n">data_mat</span><span class="o">&#39;</span><span class="p">;</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">&#34;&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-9-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</details>
<p>Now that we have our data, let‚Äôs do some inference!</p>
<h2 id="mechanistic-inference-as-a-supervised-learning-task">Mechanistic inference as a supervised learning task</h2>
<p>We‚Äôll get started with a very crude approach to inference, where we‚Äôll
treat the calibration of our LV model similarly to a supervised machine
learning task. To do so, we‚Äôll write a loss function, defining a
distance between our model and the data, and we‚Äôll try to minimize this
loss. The parameter minimizing this loss will be our best model
parameter estimate.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">loss</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">alg</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">p</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">saveat</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">abstol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">reltol</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="mf">0.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">l</span> <span class="o">+=</span> <span class="n">sum</span><span class="p">(</span><span class="n">abs2</span><span class="p">,</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">data_mat</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">l</span><span class="p">,</span> <span class="n">predicted</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>loss (generic function with 1 method)
</code></pre>
<blockquote>
<p><strong>Note</strong></p>
<p>Notice that we explicitly check whether predictions are &gt; 0, because
the log of negative number is not defined and will throw an error!</p>
</blockquote>
<p>Let‚Äôs define a helper function, that will plot how good does the model
perform across different iterations.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">callback</span> <span class="o">=</span> <span class="k">function</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">pred</span><span class="p">;</span> <span class="n">doplot</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">push!</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">length</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span><span class="o">%</span><span class="mi">100</span><span class="o">==</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="n">println</span><span class="p">(</span><span class="s">&#34;Current loss after </span><span class="si">$</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span><span class="s"> iterations: </span><span class="si">$</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="k">end</span><span class="p">])</span><span class="s">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">doplot</span>
</span></span><span class="line"><span class="cl">            <span class="n">plt</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">tsteps</span><span class="p">,</span> <span class="n">data_mat</span><span class="o">&#39;</span><span class="p">,</span>  <span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s">&#34;Prey abundance data&#34;</span> <span class="s">&#34;Predator abundance data&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">plot!</span><span class="p">(</span><span class="n">plt</span><span class="p">,</span> <span class="n">tsteps</span><span class="p">,</span> <span class="n">pred</span><span class="o">&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s">&#34;Inferred prey abundance&#34;</span> <span class="s">&#34;Inferred predator abundance&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">display</span><span class="p">(</span><span class="n">plot</span><span class="p">(</span><span class="n">plt</span><span class="p">,</span> <span class="n">yaxis</span> <span class="o">=</span> <span class="ss">:log10</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">&#34;it. : </span><span class="si">$</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span><span class="s">&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">false</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>#13 (generic function with 1 method)
</code></pre>
<p>And let‚Äôs define a wrong initial guess for the parameters</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">pinit</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">Œ±</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">Œ≤</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">Œ≥</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Œ¥</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">callback</span><span class="p">(</span><span class="n">pinit</span><span class="p">,</span> <span class="n">loss</span><span class="p">(</span><span class="n">pinit</span><span class="p">)</span><span class="o">...</span><span class="p">;</span> <span class="n">doplot</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Current loss after 1 iterations: 251.10349846646116
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-13-output-2.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<pre><code>false
</code></pre>
<p>Our initial predictions are bad, but you‚Äôll likely get even worse
predictions in a real-case scenario!</p>
<p>We‚Äôll use the library <code>Optimization</code>, which is a wrapper library around
many optimization libraries in Julia. <code>Optimization</code> therefore provides
us with many different types of optimizers to find parameters minimizing
<code>loss</code>. We‚Äôll specifically use the infamous <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam
optimizer</a> (187k citations!!!), widely
used in ML.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Optimization</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">OptimizationOptimisers</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">SciMLSensitivity</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">adtype</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">AutoZygote</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">optf</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">OptimizationFunction</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">adtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">optprob</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">OptimizationProblem</span><span class="p">(</span><span class="n">optf</span><span class="p">,</span> <span class="n">pinit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@time</span> <span class="n">res_ada</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">optprob</span><span class="p">,</span> <span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">callback</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">res_ada</span><span class="o">.</span><span class="n">minimizer</span>
</span></span></code></pre></div><pre><code>Current loss after 101 iterations: 8.039887486778179
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-14-output-2.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<pre><code>Current loss after 201 iterations: 7.9094080306025445
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-14-output-4.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<pre><code>Current loss after 301 iterations: 7.806219868794404
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-14-output-6.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<pre><code>Current loss after 401 iterations: 7.74345616951535
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-14-output-8.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<pre><code>Current loss after 501 iterations: 7.712910946192632
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-14-output-10.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<pre><code> 13.731183 seconds (49.62 M allocations: 3.145 GiB, 7.17% gc time, 93.45% compilation time: 8% of which was recompilation)

ComponentVector{Float64}(Œ± = 1.5322556800023097, Œ≤ = 1.0159023620691514, Œ≥ = 2.8926590524331766, Œ¥ = 0.9148575218436299)
</code></pre>
<p>Nice! It seems that the optimizer did a reasonable job, and that we
found a reasonable estimate of our parameters!</p>
<blockquote>
<p><strong>Exercise: Hey, this is cheating!</strong></p>
<p>Notice that we use the true <code>u0</code>, as if we were to know exactly the
initial state. In a real situation, we need also to infer the true
state!</p>
<p>Can you modify the model to infer the true state?</p>
</blockquote>
<details class="code-fold">
<summary>
Solution
</summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">loss2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">alg</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">u0</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">u0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">saveat</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">abstol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">reltol</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="mf">0.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">l</span> <span class="o">+=</span> <span class="n">sum</span><span class="p">(</span><span class="n">abs2</span><span class="p">,</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">data_mat</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">l</span><span class="p">,</span> <span class="n">predicted</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">pinit</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">Œ±</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">Œ≤</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">Œ≥</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Œ¥</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">u0</span> <span class="o">=</span> <span class="n">data_mat</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">adtype</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">AutoZygote</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">optf</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">OptimizationFunction</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">loss2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">adtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">optprob</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">OptimizationProblem</span><span class="p">(</span><span class="n">optf</span><span class="p">,</span> <span class="n">pinit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nd">@time</span> <span class="n">res_ada</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">optprob</span><span class="p">,</span> <span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">callback</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">res_ada</span><span class="o">.</span><span class="n">minimizer</span>
</span></span></code></pre></div><pre><code>Current loss after 1 iterations: 416.2139476098838
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-2.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 101 iterations: 8.276915907364208
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-4.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 201 iterations: 7.932781156086005
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-6.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 301 iterations: 7.826220840461579
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-8.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 401 iterations: 7.742200328964401
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-10.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 501 iterations: 7.6847707674856744
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-12.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 601 iterations: 7.649835853033301
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-14.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 701 iterations: 7.6304539871467085
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-16.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 801 iterations: 7.620491408711084
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-18.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 901 iterations: 7.61570935872972
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-20.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 1001 iterations: 7.61357485440162
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-15-output-22.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

6.735983 seconds (36.27 M allocations: 2.207 GiB, 4.93% gc time, 72.28% compilation time)
ComponentVector{Float64}(Œ± = 1.4627582443041978, Œ≤ = 0.9327814276650684, Œ≥ = 3.084479105946653, Œ¥ = 0.9916501731843601, u0 = [1.9639554456506427, 2.145084576010591])</p>
</details>
<h2 id="regularization">Regularization</h2>
<p>In supervised learning, it is common practice to regularize the model to
prevent overfitting. Regularization can also help the model to converge.
Regularization is done by adding a penalty term to the loss function:</p>
<p>Loss(<em>Œ∏</em>)‚ÄÑ=‚ÄÑLoss<sub>data</sub>(<em>Œ∏</em>)‚ÄÖ+‚ÄÖ<em>Œª</em>‚ÄÜReg(<em>Œ∏</em>)</p>
<blockquote>
<p><strong>Exercise: regularization</strong></p>
<p>Add a regularization term to the loss, which penalizes the loss when
the inferred initial conditions are less than 0.</p>
</blockquote>
<h2 id="multiple-shooting">Multiple shooting</h2>
<p>Another trick that can greatly improve the convergence of the
optimization is to break down the prediction task into simpler tasks.
Namely, instead of trying to predict in one shot the whole time series,
the idea of multiple shooting is to predict for shorter time horizon.</p>
<blockquote>
<p><strong>Exercise: multiple shooting</strong></p>
<p>Can you modify your loss function to implement this idea?</p>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">multiple_shooting_idx</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">length_interval</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">K</span> <span class="o">=</span> <span class="n">N</span> <span class="o">√∑</span> <span class="n">length_interval</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@assert</span> <span class="n">N</span> <span class="o">%</span> <span class="n">K</span> <span class="o">==</span> <span class="mi">1</span> <span class="s">&#34;`N - 1` is not a multiple of `length_interval`&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">interval_idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="o">*</span><span class="n">length_interval</span><span class="o">+</span><span class="mi">1</span><span class="o">:</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">length_interval</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">k</span> <span class="k">in</span> <span class="mi">0</span><span class="o">:</span><span class="p">(</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">interval_idxs</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">loss_multiple_shooting</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">interval_idxs</span> <span class="o">=</span> <span class="n">multiple_shooting_idx</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">tsteps</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="mf">0.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx</span> <span class="k">in</span> <span class="n">interval_idxs</span>
</span></span><span class="line"><span class="cl">        <span class="n">saveat</span> <span class="o">=</span> <span class="n">tsteps</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c"># u0_i = sol_true.u[idx[1]] # here we are cheating, using true states for initial conditions!</span>
</span></span><span class="line"><span class="cl">        <span class="n">u0_i</span> <span class="o">=</span> <span class="n">data_mat</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="c"># this is not cheating, but it does not work very well</span>
</span></span><span class="line"><span class="cl">        <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">alg</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">u0</span> <span class="o">=</span> <span class="n">u0_i</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">p</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">saveat</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">tspan</span><span class="o">=</span><span class="p">(</span><span class="n">saveat</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">saveat</span><span class="p">[</span><span class="k">end</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">                        <span class="n">abstol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">reltol</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">l</span> <span class="o">+=</span> <span class="n">sum</span><span class="p">(</span><span class="n">abs2</span><span class="p">,</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">data_mat</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span> <span class="o">-</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">            <span class="k">end</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">alg</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">saveat</span><span class="o">=</span><span class="n">tsteps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">abstol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">reltol</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">l</span><span class="p">,</span> <span class="n">predicted</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">pinit</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">Œ±</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">Œ≤</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">Œ≥</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Œ¥</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">adtype</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">AutoZygote</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">optf</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">OptimizationFunction</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">loss_multiple_shooting</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">adtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">optprob</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">OptimizationProblem</span><span class="p">(</span><span class="n">optf</span><span class="p">,</span> <span class="n">pinit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nd">@time</span> <span class="n">res_ada</span> <span class="o">=</span> <span class="n">Optimization</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">optprob</span><span class="p">,</span> <span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">);</span> <span class="n">callback</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">res_ada</span><span class="o">.</span><span class="n">minimizer</span>
</span></span></code></pre></div><pre><code>Current loss after 1 iterations: 57.64884717929634
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-18-output-2.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 101 iterations: 15.985881478253205
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-18-output-4.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 201 iterations: 15.984751300361513
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-18-output-6.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 301 iterations: 15.984751280519914
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-18-output-8.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 401 iterations: 15.98475128052433
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-18-output-10.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

Current loss after 501 iterations: 15.984751280410928
















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-18-output-12.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>

3.995846 seconds (16.45 M allocations: 989.683 MiB, 3.27% gc time, 69.20% compilation time)
ComponentVector{Float64}(Œ± = 2.0111356895351227, Œ≤ = 1.3936359371191127, Œ≥ = 2.8416910236613444, Œ¥ = 1.031702000687222)</p>
</details>
<h2 id="sensitivity-methods">Sensitivity methods</h2>
<p>Did you wonder why do we need to load <code>SciMLSensitivity</code>? and why did we
specify <code>adtype = Optimization.AutoZygote()</code>?</p>
<p>AD comes in different flavours, with broadly two types of AD methods -
<strong>forward methods</strong> and <strong>backward methods</strong> -, and a bunch of different
implementations.</p>
<p>You can specify which ones <code>Optimization.jl</code> will use to differentiate
<code>loss</code> with <code>adtype</code>, see available options
<a href="https://docs.sciml.ai/Optimization/stable/API/ad/" target="_blank" rel="noopener">here</a>.</p>
<p>But when it comes to differentiating the <code>solve</code> function from
<code>OrdinaryDiffEq</code>, you want to use <code>AutoZygote()</code>, because when trying to
differentiate <code>solve</code>, a specific adjoint rule provided by the
<code>SciMLSensitivity</code> package will be used.</p>
<blockquote>
<p><strong>What are adjoint rules?</strong></p>
<p>These are algoirithmic rules that specify to the AD backend <em>how</em> to
calculate the derivative of a specific function.</p>
<p>If you want to know more, check-out the <a href="https://juliadiff.org/ChainRulesCore.jl/" target="_blank" rel="noopener">ChainRules.jl
documentation</a></p>
</blockquote>
<p>These adjoint rules can be specificed by the keyword <code>sensealg</code> when
calling <code>solve</code> and have been designed for best performance when
differentiating solutions of an <code>ODEProblem</code>. There exists a lot of them
(see a review <a href="https://arxiv.org/abs/2406.09699" target="_blank" rel="noopener">here</a>), and if
<code>sensealg</code> is not provided, a smart polyalgorithm is going to pick up
one for you.</p>
<p>You can have a look in the documentation
<a href="https://docs.sciml.ai/SciMLSensitivity/stable/manual/differential_equation_sensitivities/" target="_blank" rel="noopener">here</a>
for hints on how to choose an algorithm.</p>
<blockquote>
<p><strong>Exercise: benchmarking sensitivity methods</strong></p>
<p>Can you evaluate the performance of <code>ForwardDiffSensitivity()</code> and
<code>ReverseDiffAdjoint()</code>?</p>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Zygote</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">loss_sensealg</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">sensealg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">alg</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">sensealg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">p</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">u0</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">u0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">saveat</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">abstol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                        <span class="n">reltol</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="mf">0.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">l</span> <span class="o">+=</span> <span class="n">sum</span><span class="p">(</span><span class="n">abs2</span><span class="p">,</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">data_mat</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">l</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>loss_sensealg (generic function with 1 method)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">pinit</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">Œ±</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">Œ≤</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">Œ≥</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Œ¥</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">u0</span> <span class="o">=</span> <span class="n">data_mat</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="nd">@btime</span> <span class="n">Zygote</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">p</span> <span class="o">-&gt;</span> <span class="n">loss_sensealg</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ForwardDiffSensitivity</span><span class="p">()),</span> <span class="n">pinit</span><span class="p">);</span>
</span></span></code></pre></div><pre><code>  1.039 ms (14955 allocations: 896.28 KiB)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@btime</span> <span class="n">Zygote</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">p</span> <span class="o">-&gt;</span> <span class="n">loss_sensealg</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ReverseDiffAdjoint</span><span class="p">()),</span> <span class="n">pinit</span><span class="p">);</span>
</span></span></code></pre></div><pre><code>  4.904 ms (104797 allocations: 4.45 MiB)
</code></pre>
</details>
<p>For a small number of parameters, forward methods tend to perform best,
but with higher number of parameters, the other way around is true.</p>
<p>Well done! Now, let‚Äôs jump into the Bayesian world‚Ä¶</p>
<h2 id="bayesian-inference">Bayesian inference</h2>
<p>Julia has a very strong library for Bayesian inference:
<a href="https://turinglang.org" target="_blank" rel="noopener">Turing.jl</a>.</p>
<p>Let‚Äôs declare our first Turing model!</p>
<p>This is done with the <code>@model</code> macro, which allows the library to
automatically construct the posterior distribution based on the
definition of your model‚Äôs random variables.</p>
<blockquote>
<p><strong>Frequentist (supervised learning) vs.¬†Bayesian approach</strong></p>
<p>The main difference between a frequentist approach and a Bayesian
approach is that the latter considers that parameters are random
variables. Hence instead of trying to estimate a single value for the
parameters, the Bayesian will try to estimate the posterior (joint)
distribution of those parameters.</p>
<p>$$
P(\theta | \mathcal{D}) = \frac{P(\mathcal{D} | \theta) P(\theta)}{P(\mathcal{D})}
$$</p>
</blockquote>
<p>Random variables are defined with the <code>~</code> symbol.</p>
<h3 id="our-first-turing-model">Our first Turing model</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Turing</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">LinearAlgebra</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">fitlv</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Prior distributions.</span>
</span></span><span class="line"><span class="cl">    <span class="n">œÉ</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ±</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≤</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ¥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Simulate Lotka-Volterra model. </span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span> <span class="o">=</span> <span class="p">(;</span><span class="n">Œ±</span><span class="p">,</span> <span class="n">Œ≤</span><span class="p">,</span> <span class="n">Œ≥</span><span class="p">,</span> <span class="n">Œ¥</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">alg</span><span class="p">;</span> <span class="n">p</span><span class="p">,</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Observations.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">MvLogNormal</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">œÉ</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">nothing</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>fitlv (generic function with 2 methods)
</code></pre>
<p>Now we can instantiate our model, and run the inference!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">fitlv</span><span class="p">(</span><span class="n">data_mat</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># Sample 3 independent chains with forward-mode automatic differentiation (the default).</span>
</span></span><span class="line"><span class="cl"><span class="n">chain</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(),</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">;</span> <span class="n">progress</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Chains MCMC chain (1000√ó17√ó3 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 3
Samples per chain = 1000
Wall duration     = 26.64 seconds
Compute duration  = 25.3 seconds
parameters        = œÉ, Œ±, Œ≤, Œ≥, Œ¥
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ‚ãØ
      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ‚ãØ

           œÉ    0.2796    0.0195    0.0005   1721.3574   1789.3390    1.0013   ‚ãØ
           Œ±    1.4928    0.1501    0.0052    841.9730    862.7118    1.0025   ‚ãØ
           Œ≤    0.9902    0.1210    0.0040    907.2968    995.0953    1.0008   ‚ãØ
           Œ≥    2.9967    0.2656    0.0090    863.2374    992.8786    1.0045   ‚ãØ
           Œ¥    0.9592    0.1043    0.0034    939.4457   1141.4992    1.0034   ‚ãØ
                                                                1 column omitted

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           œÉ    0.2449    0.2656    0.2788    0.2922    0.3212
           Œ±    1.2283    1.3875    1.4784    1.5863    1.8276
           Œ≤    0.7748    0.9077    0.9816    1.0661    1.2575
           Œ≥    2.4786    2.8192    2.9973    3.1727    3.5358
           Œ¥    0.7563    0.8869    0.9584    1.0274    1.1650
</code></pre>
<blockquote>
<p><strong>Threads</strong></p>
<p>How many threads do you have running? <code>Threads.nthreads()</code> will tell
you!</p>
</blockquote>
<p>Let‚Äôs see if our chains have converged.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">StatsPlots</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-26-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h3 id="data-retrodiction">Data retrodiction</h3>
<p>Let‚Äôs now generate simulated data using samples from the posterior
distribution, and compare to the original data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">plot_predictions</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">sol</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">myplot</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(;</span> <span class="n">legend</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">chain</span><span class="p">[[</span><span class="ss">:Œ±</span><span class="p">,</span> <span class="ss">:Œ≤</span><span class="p">,</span> <span class="ss">:Œ≥</span><span class="p">,</span> <span class="ss">:Œ¥</span><span class="p">]],</span> <span class="mi">300</span><span class="p">;</span> <span class="n">replace</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">parr</span> <span class="k">in</span> <span class="n">eachrow</span><span class="p">(</span><span class="kt">Array</span><span class="p">(</span><span class="n">posterior_samples</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="kt">NamedTuple</span><span class="p">([</span><span class="ss">:Œ±</span><span class="p">,</span> <span class="ss">:Œ≤</span><span class="p">,</span> <span class="ss">:Œ≥</span><span class="p">,</span> <span class="ss">:Œ¥</span><span class="p">]</span> <span class="o">.=&gt;</span> <span class="n">parr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">sol_p</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">Tsit5</span><span class="p">();</span> <span class="n">p</span><span class="p">,</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plot!</span><span class="p">(</span><span class="n">sol_p</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#34;#BBBBBB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Plot simulation and noisy observations.</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot!</span><span class="p">(</span><span class="n">sol</span><span class="p">;</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scatter!</span><span class="p">(</span><span class="n">sol</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="n">data_mat</span><span class="o">&#39;</span><span class="p">;</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">myplot</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="n">plot_predictions</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">sol_true</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-27-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<blockquote>
<p><strong>Exercise: Hey, this is cheating!</strong></p>
<p>Notice that we use the true <code>u0</code>, as if we were to know exactly the
initial state. In a real situation, we need also to infer the true
state!</p>
<p>Can you modify the model to infer the true state?</p>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">fitlv2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Prior distributions.</span>
</span></span><span class="line"><span class="cl">    <span class="n">œÉ</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ±</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≤</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ¥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">u0</span> <span class="o">~</span> <span class="n">MvLogNormal</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">œÉ</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Simulate Lotka-Volterra model but save only the second state of the system (predators).</span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span> <span class="o">=</span> <span class="p">(;</span><span class="n">Œ±</span><span class="p">,</span> <span class="n">Œ≤</span><span class="p">,</span> <span class="n">Œ≥</span><span class="p">,</span> <span class="n">Œ¥</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">alg</span><span class="p">;</span> <span class="n">p</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Observations.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">2</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">MvLogNormal</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">œÉ</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">nothing</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="n">model2</span> <span class="o">=</span> <span class="n">fitlv2</span><span class="p">(</span><span class="n">data_mat</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c"># Sample 3 independent chains.</span>
</span></span><span class="line"><span class="cl"><span class="n">chain2</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model2</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(),</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">3</span><span class="p">;</span> <span class="n">progress</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain2</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-28-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</details>
<p>Here is a small utility function to visualize your results.</p>
<details class="code-fold">
<summary>`plot_predictions2`</summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">plot_predictions2</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">sol</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">myplot</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(;</span> <span class="n">legend</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="mi">300</span><span class="p">;</span> <span class="n">replace</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">posterior_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ps</span> <span class="o">=</span> <span class="n">posterior_samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">get</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="p">[</span><span class="ss">:Œ±</span><span class="p">,</span> <span class="ss">:Œ≤</span><span class="p">,</span> <span class="ss">:Œ≥</span><span class="p">,</span> <span class="ss">:Œ¥</span><span class="p">],</span> <span class="n">flatten</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">u0</span> <span class="o">=</span> <span class="n">get</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="ss">:u0</span><span class="p">,</span> <span class="n">flatten</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">u0</span> <span class="o">=</span> <span class="p">[</span><span class="n">u0</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">u0</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">sol_p</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">Tsit5</span><span class="p">();</span> <span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plot!</span><span class="p">(</span><span class="n">sol_p</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#34;#BBBBBB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Plot simulation and noisy observations.</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot!</span><span class="p">(</span><span class="n">sol</span><span class="p">;</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scatter!</span><span class="p">(</span><span class="n">sol</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="n">data_mat</span><span class="o">&#39;</span><span class="p">;</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">myplot</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plot_predictions2</span><span class="p">(</span><span class="n">chain2</span><span class="p">,</span> <span class="n">sol_true</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-30-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</details>
<h3 id="mode-estimation">Mode estimation</h3>
<p>Turing allows you to find the maximum likelihood estimate (MLE) or
maximum a posteriori estimate (MAP).</p>
<p>$$
\theta_{MLE} = \underset{\theta}{\text{argmax}} \ P(\mathcal{D} | \theta), \qquad \theta_{MAP} = \underset{\theta}{\text{argmax}} \ P(\theta | \mathcal{D}).
$$</p>
<blockquote>
<p><strong>MAP and regularization in supervised learning</strong></p>
<p>Although Bayesian inference seems very different from the supervised
learning approach we developed in the first part, estimating the MAP,
which can be still considered as Bayesian inference, transforms in an
optimization problem that can be seen as a supervised task.</p>
<p>To see that, we can log-transform the posterior:</p>
<p>log‚ÄÜ<em>P</em>(<em>Œ∏</em>|ùíü)‚ÄÑ=‚ÄÑlog‚ÄÜ<em>P</em>(ùíü|<em>Œ∏</em>)‚ÄÖ+‚ÄÖlog‚ÄÜ<em>P</em>(<em>Œ∏</em>)‚ÄÖ‚àí‚ÄÖlog‚ÄÜ<em>P</em>(ùíü)</p>
<p>Since the evidence <em>P</em>(ùíü) is independent of <em>Œ∏</em>, it can be ignored
when maximizing with respect to <em>Œ∏</em>. Therefore, the MAP estimate
simplifies to:</p>
<p>$$
\theta_{MAP} = \underset{\theta}{\text{argmax}} \ \left[\log P(\mathcal{D} | \theta) + \log P(\theta)\right]
$$</p>
<p>Here, log‚ÄÜ<em>P</em>(ùíü|<em>Œ∏</em>) can be seen as our previous non-regularized
<code>loss</code> and log‚ÄÜ<em>P</em>(<em>Œ∏</em>) acts as a regularization term, penalizing
unlikely parameter values based on our prior beliefs. Priors on
parameters can be seen as regularization term.</p>
</blockquote>
<p>MLE and MAP can be obtained by <code>maximum_likelihood</code> and
<code>maximum_a_posteriori</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">maximum_a_posteriori</span><span class="p">(</span><span class="n">model2</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>ModeResult with maximized lp of -104.88
[0.3545376205457767, 1.4695692517420373, 0.9162499950736273, 3.263944963496157, 1.0243607922108577, 2.150749205538098, 2.4795481828054595]
</code></pre>
<p>Since <code>Turing</code> uses under the hood the same Optimization.jl library, you
can specify which optimizer youd‚Äôd like to use.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">map_res</span> <span class="o">=</span> <span class="n">maximum_a_posteriori</span><span class="p">(</span><span class="n">model2</span><span class="p">,</span> <span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>ModeResult with maximized lp of -104.88
[0.35455374965749115, 1.4707686527453756, 0.9171941147556801, 3.2614628620071664, 1.0235193248242322, 2.1506473758409883, 2.4789084651090993]
</code></pre>
<p>We can check whether the optimization has converged:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@show</span> <span class="n">map_res</span><span class="o">.</span><span class="n">optim_result</span>
</span></span></code></pre></div><pre><code>map_res.optim_result = retcode: Default
u: [-1.036895323466616, -0.05847935462336067, -0.16599185850450063, 1.1190957778225292, 0.04704732580671333, 0.765768901858866, 0.9078183282523818]
Final objective value:     104.87762402604213

retcode: Default
u: 7-element Vector{Float64}:
 -1.036895323466616
 -0.05847935462336067
 -0.16599185850450063
  1.1190957778225292
  0.04704732580671333
  0.765768901858866
  0.9078183282523818
</code></pre>
<p>What‚Äôs very nice is that Turing.jl provides you with utility functions
to analyse your mode estimation results.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">StatsBase</span>
</span></span><span class="line"><span class="cl"><span class="n">coeftable</span><span class="p">(</span><span class="n">map_res</span><span class="p">)</span>
</span></span></code></pre></div><table>
<colgroup>
<col style="width: 9%" />
<col style="width: 13%" />
<col style="width: 16%" />
<col style="width: 13%" />
<col style="width: 17%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Coef.</th>
<th style="text-align: right;">Std. Error</th>
<th style="text-align: right;">z</th>
<th style="text-align: right;">Pr(&gt;</th>
<th style="text-align: right;">z</th>
<th style="text-align: right;">)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">œÉ</td>
<td style="text-align: left;">0.354554</td>
<td style="text-align: right;">0.0250558</td>
<td style="text-align: right;">14.1506</td>
<td style="text-align: right;">1.85249e-45</td>
<td style="text-align: right;">0.305445</td>
<td style="text-align: right;">0.403662</td>
</tr>
<tr class="even">
<td style="text-align: left;">Œ±</td>
<td style="text-align: left;">1.47077</td>
<td style="text-align: right;">0.157711</td>
<td style="text-align: right;">9.32571</td>
<td style="text-align: right;">1.10241e-20</td>
<td style="text-align: right;">1.16166</td>
<td style="text-align: right;">1.77988</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Œ≤</td>
<td style="text-align: left;">0.917194</td>
<td style="text-align: right;">0.125103</td>
<td style="text-align: right;">7.3315</td>
<td style="text-align: right;">2.27594e-13</td>
<td style="text-align: right;">0.671996</td>
<td style="text-align: right;">1.16239</td>
</tr>
<tr class="even">
<td style="text-align: left;">Œ≥</td>
<td style="text-align: left;">3.26146</td>
<td style="text-align: right;">0.335101</td>
<td style="text-align: right;">9.73279</td>
<td style="text-align: right;">2.18526e-22</td>
<td style="text-align: right;">2.60468</td>
<td style="text-align: right;">3.91825</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Œ¥</td>
<td style="text-align: left;">1.02352</td>
<td style="text-align: right;">0.124744</td>
<td style="text-align: right;">8.20497</td>
<td style="text-align: right;">2.30644e-16</td>
<td style="text-align: right;">0.779026</td>
<td style="text-align: right;">1.26801</td>
</tr>
<tr class="even">
<td style="text-align: left;">u0[1]</td>
<td style="text-align: left;">2.15065</td>
<td style="text-align: right;">0.18462</td>
<td style="text-align: right;">11.6491</td>
<td style="text-align: right;">2.31953e-31</td>
<td style="text-align: right;">1.7888</td>
<td style="text-align: right;">2.51249</td>
</tr>
<tr class="odd">
<td style="text-align: left;">u0[2]</td>
<td style="text-align: left;">2.47891</td>
<td style="text-align: right;">0.247352</td>
<td style="text-align: right;">10.0218</td>
<td style="text-align: right;">1.22272e-23</td>
<td style="text-align: right;">1.99411</td>
<td style="text-align: right;">2.96371</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Exercise: Partially observed state</strong></p>
<p>Let‚Äôs assume the following situation: for some reason, you only have
observation data for the predator. Could you still infer all
parameters of your model, including those of the prey?</p>
<p>Could be! Because the signal of the variation in abundance of the
predator contains information on the dynamics of the whole
predator-prey system.</p>
<p>Do it!</p>
<p>You‚Äôll need to assume so prior state for the prey. Just assume that it
is the same as that of the predator.</p>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">fitlv3</span><span class="p">(</span><span class="n">data</span><span class="o">::</span><span class="kt">AbstractVector</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Prior distributions.</span>
</span></span><span class="line"><span class="cl">    <span class="n">œÉ</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ±</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≤</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ¥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">u0</span> <span class="o">~</span> <span class="n">MvLogNormal</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">œÉ</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Simulate Lotka-Volterra model but save only the second state of the system (predators).</span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span> <span class="o">=</span> <span class="p">(;</span><span class="n">Œ±</span><span class="p">,</span> <span class="n">Œ≤</span><span class="p">,</span> <span class="n">Œ≥</span><span class="p">,</span> <span class="n">Œ¥</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">Tsit5</span><span class="p">();</span> <span class="n">p</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">saveat</span><span class="p">,</span> <span class="n">save_idxs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Observations of the predators.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">2</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">LogNormal</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">œÉ</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">nothing</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="n">model3</span> <span class="o">=</span> <span class="n">fitlv3</span><span class="p">(</span><span class="n">data_mat</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c"># Sample 3 independent chains.</span>
</span></span><span class="line"><span class="cl"><span class="n">chain3</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model3</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(),</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">3</span><span class="p">;</span> <span class="n">progress</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">chain3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">p</span> <span class="o">=</span> <span class="n">plot_predictions2</span><span class="p">(</span><span class="n">chain3</span><span class="p">,</span> <span class="n">sol_true</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plot!</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="ss">:log10</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-35-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</details>
<p>Now you need to realise that up to now, we had a relatively simple model. How would this model scale, should we have a much larger model? Let&rsquo;s cook-up some idealised LV model. &ndash;&gt;</p>
<h3 id="ad-backends-and-sensealg">AD backends and <code>sensealg</code></h3>
<p>The <code>NUTS</code> sampler uses automatic differentiation under the hood.</p>
<p>By default, <code>Turing.jl</code> uses <code>ForwardDiff.jl</code> as an AD backend, meaning
that the SciML sensitivity methods are not used when the <code>solve</code>
function is called. However, you could change the AD backend to <code>Zygote</code>
with <code>adtype=AutoZygote()</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">chain2</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model2</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">(),</span> <span class="n">MCMCThreads</span><span class="p">(),</span> <span class="n">adtype</span><span class="o">=</span><span class="n">AutoZygote</span><span class="p">(),</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">3</span><span class="p">;</span> <span class="n">progress</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Chains MCMC chain (3000√ó19√ó3 Array{Float64, 3}):

Iterations        = 1001:1:4000
Number of chains  = 3
Samples per chain = 3000
Wall duration     = 57.41 seconds
Compute duration  = 56.94 seconds
parameters        = œÉ, Œ±, Œ≤, Œ≥, Œ¥, u0[1], u0[2]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ‚ãØ
      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ‚ãØ

           œÉ    0.3690    0.0267    0.0003   6634.3954   6080.5757    1.0000   ‚ãØ
           Œ±    1.5026    0.1596    0.0030   2825.0996   3566.4279    1.0004   ‚ãØ
           Œ≤    0.9458    0.1303    0.0024   3055.7314   3652.9872    1.0015   ‚ãØ
           Œ≥    3.2448    0.3214    0.0060   2806.7123   2850.3476    1.0009   ‚ãØ
           Œ¥    1.0199    0.1212    0.0022   3167.1794   3679.8852    1.0008   ‚ãØ
       u0[1]    2.1903    0.2017    0.0026   6066.7978   5252.2598    1.0001   ‚ãØ
       u0[2]    2.4814    0.2547    0.0034   5638.9388   5057.7901    1.0007   ‚ãØ
                                                                1 column omitted

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           œÉ    0.3219    0.3507    0.3675    0.3854    0.4254
           Œ±    1.2290    1.3890    1.4889    1.6003    1.8558
           Œ≤    0.7245    0.8536    0.9332    1.0234    1.2380
           Œ≥    2.6101    3.0243    3.2492    3.4636    3.8759
           Œ¥    0.7863    0.9365    1.0169    1.1007    1.2611
       u0[1]    1.8288    2.0508    2.1789    2.3149    2.6257
       u0[2]    2.0080    2.3053    2.4727    2.6454    3.0176
</code></pre>
<p>Doing so, you could specify within <code>solve</code> the <code>adtype</code>. It is usually a
good idea to try a few different sensitivity algorithm.</p>
<p>See
<a href="https://turinglang.org/docs/tutorials/docs-10-using-turing-autodiff/index.html" target="_blank" rel="noopener">here</a>
for more information.</p>
<blockquote>
<p><strong>Exercise: benchmark</strong></p>
<p>Can you evaluate the performance of <code>ForwardDiffSensitivity()</code> and
<code>ReverseDiffAdjoint()</code>?</p>
</blockquote>
<h3 id="variational-inference">Variational Inference</h3>
<p>Variational inference (VI) consists in approximating the true posterior
distribution <em>P</em>(<em>Œ∏</em>|ùíü) by an approximate distribution <em>Q</em>(<em>Œ∏</em>;‚ÄÜ<em>œï</em>),
where <em>œï</em> is a parameter vector defining the shape, location, and other
characteristics of the approximate distribution <em>Q</em>, to be optimzed so
that <em>Q</em> is as close as possible to <em>P</em>. This is achieved by minimizing
the Kullback-Leibler (KL) divergence between the true posterior
<em>P</em>(<em>Œ∏</em>|ùíü) and the approximate distribution :</p>
<p>$$
\phi^* = \underset{\phi}{\text{argmin}} \ \text{KL}\left(Q(\theta; \phi) \||\ P(\theta | \mathcal{D})\right)
$$</p>
<p>The advantage of VI over traditional MCMC sampling methods is that VI is
generally faster and more scalable to large datasets, as it transforms
the inference problem into an optimization problem.</p>
<p>Let‚Äôs do VI in Turing!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">import</span> <span class="n">Flux</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Turing</span><span class="o">:</span> <span class="n">Variational</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">fitlv2</span><span class="p">(</span><span class="n">data_mat</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">q0</span> <span class="o">=</span> <span class="n">Variational</span><span class="o">.</span><span class="n">meanfield</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">advi</span> <span class="o">=</span> <span class="n">ADVI</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10_000</span><span class="p">)</span> <span class="c"># first arg is the </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">q</span> <span class="o">=</span> <span class="n">vi</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">advi</span><span class="p">,</span> <span class="n">q0</span><span class="p">;</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Flux</span><span class="o">.</span><span class="n">ADAM</span><span class="p">(</span><span class="mf">1e-2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">plot_predictions_vi</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">sol</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">myplot</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(;</span> <span class="n">legend</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">parr</span> <span class="k">in</span> <span class="n">eachcol</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="kt">NamedTuple</span><span class="p">([</span><span class="ss">:Œ±</span><span class="p">,</span> <span class="ss">:Œ≤</span><span class="p">,</span> <span class="ss">:Œ≥</span><span class="p">,</span> <span class="ss">:Œ¥</span><span class="p">]</span> <span class="o">.=&gt;</span> <span class="n">parr</span><span class="p">[</span><span class="mi">2</span><span class="o">:</span><span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">u0</span> <span class="o">=</span> <span class="n">parr</span><span class="p">[</span><span class="mi">6</span><span class="o">:</span><span class="mi">7</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">sol_p</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">Tsit5</span><span class="p">();</span> <span class="n">u0</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plot!</span><span class="p">(</span><span class="n">sol_p</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#34;#BBBBBB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Plot simulation and noisy observations.</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot!</span><span class="p">(</span><span class="n">sol</span><span class="p">;</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scatter!</span><span class="p">(</span><span class="n">sol</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="n">data_mat</span><span class="o">&#39;</span><span class="p">;</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">myplot</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plot_predictions_vi</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">sol_true</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-39-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>The cool thing with VI that we can sample from the resulting <code>q</code> with
ease.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">q</span> <span class="k">isa</span> <span class="n">MultivariateDistribution</span>
</span></span></code></pre></div><pre><code>true
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">rand</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>7-element Vector{Float64}:
 0.3910702850249754
 1.8261988965103004
 1.169798842596696
 2.80613428184438
 0.8402820867003005
 2.313112765625009
 2.406422925384114
</code></pre>
<ul>
<li><a href="https://turinglang.org/docs/tutorials/09-variational-inference/" target="_blank" rel="noopener">Learn more on VI in turing
here</a></li>
<li><a href="https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html" target="_blank" rel="noopener">VI in general
here</a></li>
</ul>
<h1 id="infering-functional-forms">Infering functional forms</h1>
<p>Up to now, we have been infering the value of the model‚Äôs parameters,
assuming that the structure of our model is correct. But this is very
idealistic, specifically in ecology. As a general trend, we have little
idea of how does e.g.¬†<a href="https://en.wikipedia.org/wiki/Functional_response" target="_blank" rel="noopener">the functional response of a
species</a> look like.</p>
<p>What if instead of inferring parameter values, we could infer functional
forms, or components within our model for which we have little idea on
how to express it mathematically?</p>
<p>In Julia, we can do that.</p>
<p>To illustrate this, we‚Äôll assume that we do not know the functional
response of both prey and predator, i.e.¬†the terms <code>Œ≤ * y</code> and <code>Œ¥ * x</code>.
Instead, we will parametrize this component in our DE model by a neural
network, which can be seen as a simple non-linear regressor dependent on
some extra parameters <code>p_nn</code>.</p>
<p>We then simply have to optimize those parameters, along with the other
model‚Äôs parameters!</p>
<p>Let‚Äôs get started. To make the neural network, we‚Äôll use the deep
learning library <code>Lux.jl</code>, which is similar to <code>Flux.jl</code> but where
models are explicitly parametrized. This explicit parametrization makes
it simpler to integrate with an ODE model.</p>
<p>To make things simpler, we will define a single layer neural network</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Lux</span>
</span></span><span class="line"><span class="cl"><span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rng</span> <span class="o">=</span> <span class="n">Random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">nn_init</span> <span class="o">=</span> <span class="n">Lux</span><span class="o">.</span><span class="n">Chain</span><span class="p">(</span><span class="n">Lux</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">relu</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">p_nn_init</span><span class="p">,</span> <span class="n">st_nn</span> <span class="o">=</span> <span class="n">Lux</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">nn_init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">nn</span> <span class="o">=</span> <span class="n">StatefulLuxLayer</span><span class="p">(</span><span class="n">nn_init</span><span class="p">,</span> <span class="n">st_nn</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>StatefulLuxLayer{true}(
    Dense(2 =&gt; 2, relu),                # 6 parameters
)         # Total: 6 parameters,
          #        plus 0 states.
</code></pre>
<p>We use a <code>StatefulLuxLayer</code> to not having to carry around <code>st_nn</code>, a
struct containing states of a Lux model, which is essentially useless
for a multi-layer perceptron.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">st_nn</span>
</span></span></code></pre></div><pre><code>NamedTuple()
</code></pre>
<p>We can now evaluate our neural network model as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">nn</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="n">p_nn_init</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>2-element Vector{Float64}:
 0.0
 0.0
</code></pre>
<p>instead of</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">nn_init</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="n">p_nn_init</span><span class="p">,</span> <span class="n">st_nn</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>([0.0, 0.0], NamedTuple())
</code></pre>
<p>Let‚Äôs define a new parameter vectors, which will consist of the ODE
model parameters as well as the neural net parameters</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">pinit</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">œÉ</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">Œ±</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">Œ≥</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">p_nn</span><span class="o">=</span><span class="n">p_nn_init</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>ComponentVector{Float64}(œÉ = 0.3, Œ± = 1.0, Œ≥ = 1.0, p_nn = (weight = [-1.0083649158477783 -0.7284937500953674; -1.219232201576233 0.4427390396595001], bias = [0.0; 0.0;;]))
</code></pre>
<blockquote>
<p><strong>Exercise: neural network-based Lotka-Volterra model</strong></p>
<p>Define the neural network-based Lotka-Volterra model</p>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">lotka_volterra_nn</span><span class="p">(</span><span class="n">du</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Model parameters.</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@unpack</span> <span class="n">Œ±</span><span class="p">,</span> <span class="n">Œ≥</span><span class="p">,</span> <span class="n">p_nn</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Current state.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">u</span>
</span></span><span class="line"><span class="cl">    <span class="n">uÃÇ</span> <span class="o">=</span> <span class="n">nn</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">p_nn</span><span class="p">)</span> <span class="c"># Network prediction</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Evaluate differential equations.</span>
</span></span><span class="line"><span class="cl">    <span class="n">du</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Œ±</span> <span class="o">-</span> <span class="n">uÃÇ</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">x</span> <span class="c"># prey</span>
</span></span><span class="line"><span class="cl">    <span class="n">du</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">uÃÇ</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">Œ≥</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span> <span class="c"># predator</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">nothing</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>lotka_volterra_nn (generic function with 1 method)
</code></pre>
</details>
<p>Let‚Äôs check our initial model predictions:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">prob_nn</span> <span class="o">=</span> <span class="n">ODEProblem</span><span class="p">(</span><span class="n">lotka_volterra_nn</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">tspan</span><span class="p">,</span> <span class="n">pinit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">init_sol</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob_nn</span><span class="p">,</span> <span class="n">alg</span><span class="p">;</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c"># Plot simulation.</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">init_sol</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-49-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>Now we can define our Turing Model. We‚Äôll need to use a utility function
<code>vector_to_parameters</code> that reconstructs the neural network parameter
type based on a sampled parameter vector (taken from <a href="https://quarto.org/docs/output-formats/html-code.html" target="_blank" rel="noopener">this Turing
tutorial</a>). You
do not need to worry about this. Note that we could have used a
component vector, but for some reason this did not work at the time of
the writing of this tutorial‚Ä¶</p>
<details class="code-fold">
<summary>`vector_to_parameters`</summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Functors</span> <span class="c"># for the `fmap`</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">ps_new</span><span class="o">::</span><span class="kt">AbstractVector</span><span class="p">,</span> <span class="n">ps</span><span class="o">::</span><span class="kt">NamedTuple</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@assert</span> <span class="n">length</span><span class="p">(</span><span class="n">ps_new</span><span class="p">)</span> <span class="o">==</span> <span class="n">Lux</span><span class="o">.</span><span class="n">parameterlength</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">function</span> <span class="n">get_ps</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">z</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">view</span><span class="p">(</span><span class="n">ps_new</span><span class="p">,</span> <span class="n">i</span><span class="o">:</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">+=</span> <span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">z</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">fmap</span><span class="p">(</span><span class="n">get_ps</span><span class="p">,</span> <span class="n">ps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div></details>
<pre><code>vector_to_parameters (generic function with 1 method)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="c"># Create a regularization term and a Gaussian prior variance term.</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">fitlv_nn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Prior distributions.</span>
</span></span><span class="line"><span class="cl">    <span class="n">œÉ</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ±</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">nparameters</span> <span class="o">=</span> <span class="n">Lux</span><span class="o">.</span><span class="n">parameterlength</span><span class="p">(</span><span class="n">nn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">p_nn_vec</span> <span class="o">~</span> <span class="n">MvNormal</span><span class="p">(</span><span class="n">zeros</span><span class="p">(</span><span class="n">nparameters</span><span class="p">),</span> <span class="n">sigma</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">p_nn</span> <span class="o">=</span> <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">p_nn_vec</span><span class="p">,</span> <span class="n">p_nn_init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Simulate Lotka-Volterra model. </span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span> <span class="o">=</span> <span class="p">(;</span><span class="n">Œ±</span><span class="p">,</span> <span class="n">Œ≥</span><span class="p">,</span> <span class="n">p_nn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">alg</span><span class="p">;</span> <span class="n">p</span><span class="p">,</span> <span class="n">saveat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c"># Observations.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">~</span> <span class="n">MvLogNormal</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">œÉ</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">nothing</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">fitlv_nn</span><span class="p">(</span><span class="n">data_mat</span><span class="p">,</span> <span class="n">prob_nn</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>DynamicPPL.Model{typeof(fitlv_nn), (:data, :prob), (), (), Tuple{Matrix{Float64}, ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, ComponentVector{Float64, Vector{Float64}, Tuple{Axis{(œÉ = 1, Œ± = 2, Œ≥ = 3, p_nn = ViewAxis(4:9, Axis(weight = ViewAxis(1:4, ShapedAxis((2, 2))), bias = ViewAxis(5:6, ShapedAxis((2, 1))))))}}}, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(lotka_volterra_nn), UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}, SciMLBase.StandardODEProblem}}, Tuple{}, DynamicPPL.DefaultContext}(fitlv_nn, (data = [1.8655845948955276 2.298199048573464 ‚Ä¶ 4.071164055293614 5.672667515002083; 2.651867857608795 3.2812317734519048 ‚Ä¶ 1.351784872962806 1.1243450946947573], prob = ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, ComponentVector{Float64, Vector{Float64}, Tuple{Axis{(œÉ = 1, Œ± = 2, Œ≥ = 3, p_nn = ViewAxis(4:9, Axis(weight = ViewAxis(1:4, ShapedAxis((2, 2))), bias = ViewAxis(5:6, ShapedAxis((2, 1))))))}}}, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(lotka_volterra_nn), UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}, SciMLBase.StandardODEProblem}(ODEFunction{true, SciMLBase.AutoSpecialize, typeof(lotka_volterra_nn), UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}(lotka_volterra_nn, UniformScaling{Bool}(true), nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, SciMLBase.DEFAULT_OBSERVED, nothing, nothing, nothing, nothing), [2.0, 2.0], (0.0, 5.0), (œÉ = 0.3, Œ± = 1.0, Œ≥ = 1.0, p_nn = (weight = [-1.0083649158477783 -0.7284937500953674; -1.219232201576233 0.4427390396595001], bias = [0.0; 0.0;;])), Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}(), SciMLBase.StandardODEProblem())), NamedTuple(), DynamicPPL.DefaultContext())
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">using</span> <span class="n">Optimization</span><span class="p">,</span> <span class="n">OptimizationOptimisers</span>
</span></span><span class="line"><span class="cl"><span class="nd">@time</span> <span class="n">map_res</span> <span class="o">=</span> <span class="n">maximum_a_posteriori</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ADAM</span><span class="p">(</span><span class="mf">0.05</span><span class="p">),</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">initial_params</span><span class="o">=</span><span class="n">pinit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pmap</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">œÉ</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pinit</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pmap</span> <span class="o">.=</span> <span class="n">map_res</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">:</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">sol_map</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob_nn</span><span class="p">,</span> <span class="n">alg</span><span class="p">;</span><span class="n">p</span><span class="o">=</span><span class="n">pmap</span><span class="p">,</span> <span class="n">saveat</span><span class="p">,</span> <span class="n">tspan</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">scatter</span><span class="p">(</span><span class="n">tsteps</span><span class="p">,</span> <span class="n">data_mat</span><span class="o">&#39;</span><span class="p">,</span>  <span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s">&#34;Predator abundance data&#34;</span> <span class="s">&#34;Prey abundance data&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plot!</span><span class="p">(</span><span class="n">sol_map</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s">&#34;Inferred predator abundance&#34;</span> <span class="s">&#34;Inferred prey abundance&#34;</span><span class="p">],</span> <span class="n">yscale</span><span class="o">=</span><span class="ss">:log10</span><span class="p">)</span>
</span></span></code></pre></div><pre><code> 12.103192 seconds (31.83 M allocations: 5.634 GiB, 3.78% gc time, 86.93% compilation time: &lt;1% of which was recompilation)
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-52-output-2.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>This seems to fail.</p>
<blockquote>
<p><strong>Exercise: take some initiative!</strong></p>
<p>What could you do to improve the convergence of the optimization?</p>
</blockquote>
<details class="code-fold">
<summary> Solution </summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.2</span>
</span></span><span class="line"><span class="cl"><span class="nd">@model</span> <span class="k">function</span> <span class="n">fitlv_nn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Prior distributions.</span>
</span></span><span class="line"><span class="cl">    <span class="n">œÉ</span> <span class="o">~</span> <span class="n">InverseGamma</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ±</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Œ≥</span> <span class="o">~</span> <span class="n">truncated</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">);</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">nparameters</span> <span class="o">=</span> <span class="n">Lux</span><span class="o">.</span><span class="n">parameterlength</span><span class="p">(</span><span class="n">nn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">p_nn_vec</span> <span class="o">~</span> <span class="n">MvNormal</span><span class="p">(</span><span class="n">zeros</span><span class="p">(</span><span class="n">nparameters</span><span class="p">),</span> <span class="n">sigma</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">p_nn</span> <span class="o">=</span> <span class="n">vector_to_parameters</span><span class="p">(</span><span class="n">p_nn_vec</span><span class="p">,</span> <span class="n">p_nn_init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># Simulate Lotka-Volterra model. </span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span> <span class="o">=</span> <span class="p">(;</span><span class="n">Œ±</span><span class="p">,</span> <span class="n">Œ≥</span><span class="p">,</span> <span class="n">p_nn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">interval_idxs</span> <span class="o">=</span> <span class="n">multiple_shooting_idx</span><span class="p">(</span><span class="n">length</span><span class="p">(</span><span class="n">tsteps</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">ts_idx</span> <span class="k">in</span> <span class="n">interval_idxs</span>
</span></span><span class="line"><span class="cl">        <span class="n">saveat</span> <span class="o">=</span> <span class="n">tsteps</span><span class="p">[</span><span class="n">ts_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">u0</span> <span class="o">=</span> <span class="n">sol_true</span><span class="o">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">predicted</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob_nn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">alg</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                            <span class="n">tspan</span> <span class="o">=</span> <span class="p">(</span><span class="n">saveat</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">saveat</span><span class="p">[</span><span class="k">end</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">                            <span class="n">u0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">p</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                            <span class="n">saveat</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">abstol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                            <span class="n">reltol</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c"># Observations.</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">all</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">.&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">data</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">ts_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">~</span> <span class="n">MvLogNormal</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="p">(</span><span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">œÉ</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">end</span>
</span></span><span class="line"><span class="cl">        <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">end</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">nothing</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">fitlv_nn</span><span class="p">(</span><span class="n">data_mat</span><span class="p">,</span> <span class="n">prob_nn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nd">@time</span> <span class="n">map_res</span> <span class="o">=</span> <span class="n">maximum_a_posteriori</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">initial_params</span><span class="o">=</span><span class="n">pinit</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pmap</span> <span class="o">=</span> <span class="n">ComponentArray</span><span class="p">(;</span><span class="n">œÉ</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pinit</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pmap</span> <span class="o">.=</span> <span class="n">map_res</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">:</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">sol_map</span> <span class="o">=</span> <span class="n">solve</span><span class="p">(</span><span class="n">prob_nn</span><span class="p">,</span> <span class="n">alg</span><span class="p">;</span><span class="n">p</span><span class="o">=</span><span class="n">pmap</span><span class="p">,</span> <span class="n">saveat</span><span class="p">,</span> <span class="n">tspan</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plot</span><span class="p">(</span><span class="n">sol_map</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s">&#34;Inferred predator abundance&#34;</span> <span class="s">&#34;Inferred prey abundance&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">scatter!</span><span class="p">(</span><span class="n">sol_map</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="n">data_mat</span><span class="o">&#39;</span><span class="p">,</span>  <span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="p">[</span><span class="s">&#34;Predator abundance data&#34;</span> <span class="s">&#34;Prey abundance data&#34;</span><span class="p">],</span> <span class="n">yscale</span><span class="o">=</span><span class="ss">:log10</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>  6.436015 seconds (53.74 M allocations: 23.768 GiB, 22.96% gc time, 14.78% compilation time: 75% of which was recompilation)
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-53-output-2.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</details>
<p>Happy with the convergence? Now let‚Äôs investigate what did the neural
network learn!</p>
<details class="code-fold">
<summary>`plot_func_resp`</summary>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="k">function</span> <span class="n">plot_func_resp</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c"># plotting prediction of functional response</span>
</span></span><span class="line"><span class="cl">    <span class="n">u1</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="n">minimum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">:</span><span class="p">]),</span> <span class="n">maximum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">:</span><span class="p">]),</span> <span class="n">length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">    <span class="n">u2</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="n">minimum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">:</span><span class="p">]),</span> <span class="n">maximum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">:</span><span class="p">]),</span> <span class="n">length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">    <span class="n">u</span> <span class="o">=</span> <span class="n">hcat</span><span class="p">(</span><span class="n">u1</span><span class="p">,</span><span class="n">u2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">func_resp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">(</span><span class="n">u</span><span class="o">&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">p_nn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">myplot1</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">u2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="o">-</span> <span class="n">p_true</span><span class="o">.</span><span class="n">Œ≤</span> <span class="o">.*</span> <span class="n">u2</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">label</span><span class="o">=</span><span class="s">&#34;True functional form&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">xlabel</span><span class="o">=</span><span class="s">&#34;Predator abundance&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot!</span><span class="p">(</span><span class="n">myplot1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">u2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="o">-</span> <span class="n">func_resp</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">:</span><span class="p">];</span> 
</span></span><span class="line"><span class="cl">                <span class="n">color</span><span class="o">=</span><span class="s">&#34;#BBBBBB&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">label</span><span class="o">=</span><span class="s">&#34;Inferred functional form&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">myplot2</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">u1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">p_true</span><span class="o">.</span><span class="n">Œ¥</span> <span class="o">.*</span> <span class="n">u2</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">legend</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">&#34;Prey abundance&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">plot!</span><span class="p">(</span><span class="n">myplot2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">u1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">func_resp</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">:</span><span class="p">];</span> 
</span></span><span class="line"><span class="cl">            <span class="n">color</span><span class="o">=</span><span class="s">&#34;#BBBBBB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">myplot</span> <span class="o">=</span> <span class="n">plot</span><span class="p">(</span><span class="n">myplot1</span><span class="p">,</span> <span class="n">myplot2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">myplot</span>
</span></span><span class="line"><span class="cl"><span class="k">end</span>
</span></span></code></pre></div><pre><code>plot_func_resp (generic function with 1 method)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="line"><span class="cl"><span class="n">plot_func_resp</span><span class="p">(</span><span class="n">pmap</span><span class="p">,</span> <span class="n">data_mat</span><span class="p">)</span>
</span></span></code></pre></div><p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=""
           src="/post/primer_mechanistic_inference/mechanistic_inference_1_files/figure-markdown_strict/cell-56-output-1.svg"
           loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</details>
<p>The neural network has well captured the functional forms of the
predator and prey.</p>
<blockquote>
<p><strong>Exercise: Probabilistic functional forms</strong></p>
<p>Could you try to obtain a bayesian estimate of the functional forms
with e.g.¬†VI?</p>
</blockquote>
<p>This concludes this tutorial; I hope that it has given you plenty of
ideas for your future research projects!</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="https://turinglang.org/docs/tutorials/10-bayesian-differential-equations/" target="_blank" rel="noopener">https://turinglang.org/docs/tutorials/10-bayesian-differential-equations/</a></li>
<li><a href="https://turinglang.org/docs/tutorials/09-variational-inference/" target="_blank" rel="noopener">https://turinglang.org/docs/tutorials/09-variational-inference/</a></li>
</ul>

    </div>

    




<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/scientific-machine-learning/">Scientific Machine Learning</a>
  
  <a class="badge badge-light" href="/tag/julia/">julia</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://vboussange.github.io/post/primer_mechanistic_inference/&amp;text=A%20primer%20on%20mechanistic%20inference%20with%20differentiable%20process-based%20models%20in%20Julia" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://vboussange.github.io/post/primer_mechanistic_inference/&amp;t=A%20primer%20on%20mechanistic%20inference%20with%20differentiable%20process-based%20models%20in%20Julia" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=A%20primer%20on%20mechanistic%20inference%20with%20differentiable%20process-based%20models%20in%20Julia&amp;body=https://vboussange.github.io/post/primer_mechanistic_inference/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://vboussange.github.io/post/primer_mechanistic_inference/&amp;title=A%20primer%20on%20mechanistic%20inference%20with%20differentiable%20process-based%20models%20in%20Julia" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=A%20primer%20on%20mechanistic%20inference%20with%20differentiable%20process-based%20models%20in%20Julia%20https://vboussange.github.io/post/primer_mechanistic_inference/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://vboussange.github.io/post/primer_mechanistic_inference/&amp;title=A%20primer%20on%20mechanistic%20inference%20with%20differentiable%20process-based%20models%20in%20Julia" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://vboussange.github.io"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu17fe142d2d9ea2daa3913aa53535eec6_854723_270x270_fill_q75_lanczos_center.jpg" alt="Victor Boussange"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://vboussange.github.io">Victor Boussange</a></h5>
      <h6 class="card-subtitle">Postdoctoral researcher</h6>
      <p class="card-text">Researcher in ecology and evolution, scientific machine learning enthusiastic.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:vic.boussange@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=SaQH5YoAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/vboussange" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/uploads/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0002-4202-3599" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    ¬© 2025 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> ‚Äî the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.3d946de2e8784a477845261d87025092.js"></script>

    
    
    
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/julia.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.e8fd2d733eef6a8bbbe0539398fc0547.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.de33c90527762392d440555c36739dd9.js"></script>

    
    
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js" type="module"></script>






</body>
</html>
