[{"authors":null,"categories":null,"content":"Hey there, I’m Victor, a postdoctoral researcher in the Dynamic Macroecology Group at the Swiss Federal Institute for Forest, Snow \u0026amp; Landscape (WSL), Switzerland. I am broadly interested in understanding and predicting the dynamics of complex systems by developing methods leveraging the extrapolation ability of mechanistic models with the flexibility of machine learning techniques. My main domain of application is ecology, where I focus on understanding the dynamics of ecosystems and their response to disruptions. Outside of work, I am a part-time alpinist, passionate about mountain adventures and writing. I also enjoy sailing and surfing occasionally. You can check out my alpine CV here.\n Download my resumé. -- ","date":1735776000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1735776000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hey there, I’m Victor, a postdoctoral researcher in the Dynamic Macroecology Group at the Swiss Federal Institute for Forest, Snow \u0026 Landscape (WSL), Switzerland. I am broadly interested in understanding and predicting the dynamics of complex systems by developing methods leveraging the extrapolation ability of mechanistic models with the flexibility of machine learning techniques.","tags":null,"title":"Victor Boussange","type":"authors"},{"authors":["Victor Boussange"],"categories":null,"content":"Here you will learn about different techniques to infer parameters of a (differentiable) process-based model against data. This is useful to in the context of mechanistic inference, where we want to explain patterns in a system by understanding the processes that generate them, in contrast to purely statistical or empirical inference, which might identify patterns or correlations in data without necessarily understanding the causes. We’ll mostly focus on differential equation models. Make sure that you stick to the end, where we’ll see how we can not only infer parameter values but also functional forms, by parametrizing models’ components with neural networks.\nPreliminaries Wait, what is a differentiable model? One can usually write a model as a map ℳ mapping some parameters p, an initial state u0 and a time t to a future state ut\nut = ℳ(u0, t, p).\nWe call differentiable a model ℳ for which we can calculate its derivative with respect to p or u0. The derivative $\\frac{\\partial \\mathcal{M}}{\\partial \\theta}$ expresses how much the model output changes with respect to a small change in θ.\n Recall your Calculus class!\n$$\\frac{df}{dx}(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n Let’s illustrate this concept with the logistic equation model. This model has an analytic formulation given by:\n$$\\mathcal{M}(u_0, p, t) = \\frac{K}{1 + \\big( \\frac{K-u_0}{u_0} \\big) e^{rt}}$$\nLet’s code it\nusing UnPack using Plots using Random using ComponentArrays using BenchmarkTools Random.seed!(0) function mymodel(u0, p, t) T = eltype(u0) @unpack r, K = p @. K / (one(T) + (K - u0) / u0 * exp(-r * t)) end p = ComponentArray(;r = 1., K = 1.) u0 = 0.005 tsteps = range(0, 20, length=100) y = mymodel(u0, p, tsteps) plot(tsteps, y)     What is a ComponentArray?\nA ComponentArray is a convenient Array type that allows to access array elements with symbols, similarly to a NamedTuple, while behaving like a standard array. For instance, you could do something like\ncv = ComponentVector(;a = 1, b = 2) cv .= [3, 4] ComponentVector{Int64}(a = 3, b = 4)  This is useful, because you can only calculate a gradient w.r.t a Vector!\n Now let’s try to calculate the gradient of this model. While you could in this case derive the gradient analytically, an analytic derivation is generally tricky with complex models. And what about models that can only be simulated numerically, with no analytic expressions? We need to find a more automatized way to calculate gradients.\nHow about the finite difference method?\n Exercise: finite differences\nImplement the function ∂mymodel_∂K(h, u0, p, t) which returns the model’s derivative with respect to K, calculated with a small h to be provided by the user.\n  Solution  function ∂mymodel_∂K(h, u0, p, t) phat = (; r = p.r, K= p.K + h) return (mymodel(u0, phat, t) - mymodel(u0, p, t)) / h end ∂mymodel_∂K(1e-1, u0, p, 1.) 0.00010443404854589694\n The gradient of the model is useful to understand how a parameter influences the output of the model. Let’s calculate the importance of the carrying capacity K on the model output:\ndm_dp = ∂mymodel_∂K(1e-1, u0, p, tsteps) plot(tsteps, dm_dp)    As you can observe, the carrying capacity has no effect at small t where population is small, and its influence on the dynamics grows as the population grows. We expect the reverse effect for r.\nOn the importance of gradients for inference The ability to calculate the derivative of a model is crucial when it comes to inference. Both within a full Bayesian inference context, where one wants to sample the posterior distribution of parameters θ given data u, p(θ|u), or when one wants to obtain a point estimate $\\theta^\\star = \\text{argmax}_\\theta (p(\\theta | u))$ (frequentist or machine learning context), the model gradient proves very useful. In a full Bayesian inference context, they are used e.g. with Hamiltonian Markov Chains methods, such as the NUTS sampler, and in a machine learning context, they are used with gradient-based optimizer.\nGradient descent The best way to grasp the importance of gradients in inference is to understand the gradient descent algorithm.\nThe following picture illustrates the algorithm in the special case where p is one-dimensional.\n   Given an initial estimate of the parameter value p0, $\\frac{d \\mathcal{M}}{dp}$ is used to suggest a new, better estimate, following\n$$p_{n+1} = p_n - \\eta \\frac{d \\mathcal{M}}{dp}(u_0, t, p) $$\nwhere η is the learning rate.\nGradient-based methods are usually very efficient in high-dimensional spaces.\nAutomatic differentiation Let’s go back to our method ∂mymodel_∂p. What is the optimal value of h to calculate the derivative? This is a tricky question, because a too small h can lead to round off errors (see more explanations here) while h too large also leads to a bad approximation of the asymptotic definition.\nFortunately, a bunch of techniques referred to as automatic differentiation (AD) allows to exactly differentiate any piece of numerical functions. In practice, your code …","date":1735776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735776000,"objectID":"459357d6d6c3ff066c785fbc41554f84","permalink":"https://vboussange.github.io/post/primer_mechanistic_inference/","publishdate":"2025-01-02T00:00:00Z","relpermalink":"/post/primer_mechanistic_inference/","section":"post","summary":"In this tutorial, you will learn about different techniques to infer parameters of a (differentiable) process-based model against data.","tags":["Scientific Machine Learning","Julia"],"title":"A primer on mechanistic inference with differentiable process-based models in Julia","type":"post"},{"authors":["Victor Boussange","Mauro Werder"],"categories":null,"content":"Documentation serves multiple purposes and may be useful for various audiences, including your future self, collaborators, users and contributors - should you aim at packaging some of your code into a general-purpose library.\nThis post is part of a series of posts on best practices for managing research project code. Much of this material was developed in collaboration with Mauro Werder as part of the Course On Reproducible Research, Data Pipelines, and Scientific Computing (CORDS). If you have experiences to share or spot any errors, please reach out!\nContent  Content  Style guides Comments Literal documentation  README API documentation / doc strings Type annotations Consider raising errors Tutorials   Accessing documentation Doc testing Useful packages to help you write and lint your documentation More resources Take home messages    Style guides The best documentation starts by writing self-explanatory code with good conventions.\nCorrectly naming your variables enhances code clarity.\n There are only two hard things in Computer Science: cache invalidation and naming things. Martin Fowler\n Instead of using generic names like l for a list:\nfor l in L: pass Use descriptive names like\nfor line in lines: pass Using style guides for your chosen language ensures consistency and readability in your code. Here are some resources:\n Julia style guide Google python style guide  Do not hesitate to refactor your code regularly and remove dead code to prevent confusion for yourself and others.\nComments In-line comments should be used sparingly. Aim to write self-explanatory code instead. Use comments to provide context not apparent from the code itself, such as references to papers, Stack Overflow topics, or TODOs.\nUse single-line comments for brief explanations and multi-line comments for more detailed information.\njulia\n#= This is a multi-line comment =# python\n\u0026#34;\u0026#34;\u0026#34; This is a multi-line comment \u0026#34;\u0026#34;\u0026#34; Tip: use vscode rewrap comment/text to nicely format multiline comments.\nOn top of nicely formatting your code and appending comments where necessary, a literal documentation greatly facilitates the maintenance, understandability and reproducibility of your code.\nLiteral documentation Literal documentation helps users understand your tool and get started with it.\nREADME A README file is essential for any research repository. It is displayed on under the code structure when accessing a GitHub repo. It should contain:\n (Badges showing tests, and a nice logo) A one-sentence description of your project A longer description An overview of the repository structure and files A Getting started or Examples section An Installation section with dependencies A Citation/Reference section (A link to the documentation) (A How to contribute section) An Acknowledgement section A License section  Some examples:\n SatClip GraphCast Alphafold  API documentation / doc strings API documentation describes the usage of functions, classes (types) and modules (packages). Parsers usually support markdown styles, which also enhances raw readability for humans. In short, markdown styles consists in using\n backticks ` for variable names # for titles, … See here for an introduction to markdown  Doc strings in python live inside the function\ndef best_function_ever(a_param, another_parameter): \u0026#34;\u0026#34;\u0026#34; this is the docstring \u0026#34;\u0026#34;\u0026#34; # do some stuff But above the function or type definition in Julia\n\u0026#34;\u0026#34;\u0026#34; this is the docstring \u0026#34;\u0026#34;\u0026#34; function best_function_ever(a_param, another_parameter) # do some stuff end \u0026#34;Tell whether there are too foo items in the array.\u0026#34; foo(xs::Array) = ... Best practice for docstrings include\n (in Julia: insert the signature of your function ) Short description Arguments (Args, Input,…) Returns Examples  Several flavours may be used, even for a single language.\npython 3 Different documentation style flavours\n reST (reStructuredText) Google style Numpy style  Google style is easier to read for humans\ndef add(a, b): \u0026#34;\u0026#34;\u0026#34; Add two integers. This function takes two integer arguments and returns their sum. # Parameters: a: The first integer to be added. b: The second integer to be added. # Return: int: The sum of the two integers. # Raise: TypeError: If either of the arguments is not an integer. Examples: \u0026gt;\u0026gt;\u0026gt; add(2, 3) 5 \u0026gt;\u0026gt;\u0026gt; add(-1, 1) 0 \u0026gt;\u0026gt;\u0026gt; add(\u0026#39;a\u0026#39;, 1) Traceback (most recent call last): ... TypeError: Both arguments must be integers. \u0026#34;\u0026#34;\u0026#34; if not isinstance(a, int) or not isinstance(b, int): raise TypeError(\u0026#34;Both arguments must be integers\u0026#34;) return a + b julia\n\u0026#34;\u0026#34;\u0026#34; add(a, b) Adds two integers. This function takes two integer arguments and returns their sum. # Arguments - `a`: The first integer to be added. - `b`: The second integer to be added. # Returns - The sum of the two integers. # Examples ```julia-repl julia\u0026gt; add(2, 3) 5 julia\u0026gt; add(-1, 1) 0 ``` \u0026#34;\u0026#34;\u0026#34; function add(a, b) return a + b end You may use tools like Documenter.jl or Sphinx to automatically render your API documentation on a website. Github actions can automatize the process of building the …","date":1718064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719273600,"objectID":"125b38d67bee2208d737120b0fb89e49","permalink":"https://vboussange.github.io/post/documenting-your-research-code/","publishdate":"2024-06-11T00:00:00Z","relpermalink":"/post/documenting-your-research-code/","section":"post","summary":"Documentation serves multiple purposes and may be useful for various audiences, including your future self, collaborators, users and contributors - should you aim at packaging some of your code into a general-purpose library.","tags":["julia","python"],"title":"A multi-language overview on how to document your research project code","type":"post"},{"authors":["Victor Boussange","Mauro Werder"],"categories":null,"content":"Your future self and others should be able to recreate the minimal environment to run the scripts in your research project. This is best achieved using package managers and virtual environments.\nThis post is part of a series of posts on best practices for managing research project code. Much of this material was developed in collaboration with Mauro Werder as part of the Course On Reproducible Research, Data Pipelines, and Scientific Computing (CORDS). If you have experiences to share or spot any errors, please reach out!\nContent  Content Some definitions  What is a dependency? What is a package manager? What is a virtual environment?   Package managers  Multilanguage overview  conda renv Pkg   Environment files  Julia Python R   Working with interactive environments Caveats of virtual environments   Advanced topic: package development Take-home messages  Some definitions What is a dependency? A dependency is an external package that a project requires to run.\nWhat is a package manager? A package manager like conda, Pkg or renv automates the process of installing, upgrading, configuring, and managing dependencies. It usually relies on a package repository, which is a central location that stores in one place the source code of packages or where to find it.\nWhat is a virtual environment? A virtual environment is an isolated environment where you can install and manage dependencies separately from the system-wide installation. This isolation ensures that different projects can have different dependencies and versions of packages without causing conflicts. Why use a virtual environment?\n For yourself, to best deal with multiple projects and to prevent your code from breaking down overtime.  Without specifying a virtual environment, you install packages in your base environment, which is shared across all your projects. Imagine you are working with Project A and Project B, which both depend on Package1 (currently @v1.1). You leave aside Project A for a few months, and focus on Project B. A new feature in Package1 motivate you to upgrade to v1.2, which modifies the API or the behavior of one function used in both projects. You then want to come back to Project A, but now everything is broken! Because your code has been formatted to work with Package1@v1.1. Hence, you want to make sure to compartmentalize environments.   To share your environment with others individuals and machines.  A virtual environement tracks the minimum dependencies, which can easily be shared and installed on other machines (e.g., a HPC).    Package managers Multilanguage overview     Python R Julia     Package Manager pip, conda (see also mamba), poetry install.packages() (base R) Pkg   Package Repository PyPI (Python Package Index), conda-forge CRAN (Comprehensive R Archive Network) General registry   Distribution Format .whl (wheel, incl binaries) or tar.gz (source) .tar.gz (source and/or binary) Pkg will git clone from source, and download (binary) artifacts   Virtual Environment venv, virtualenv, conda env renv Built-in in the Pkg module   Dependency Management requirements.txt or Pipfile (pip), or environment.yml (conda env) or pyproject.toml (poetry) DESCRIPTION, NAMESPACE Project.toml, Manifest.toml, Artifacts.toml    This table is very much inspired by The Scientific Coder article on package managers.\nJulia or R have built-in package managers which can be called within the REPL but Python package managers are called from outside the language.\nconda conda is a very appropriate package manager for scientific projects in Python. Over its older concurrent pip, it can handle python versions and all sorts non-python dependencies artifacts. With two lines of code, it allows someone to quickly install the virtual environment, without any pre-requiste python installation.\nHere are some essential conda commands.\nconda create --name myenv # creates new virtual environment conda activate myenv # activate the environment conda install numpy -c conda-forge # install a package conda deactivate Note that not using -c conda-forge will do just fine, but what is it? conda-forge is a community-driven channel (repository in the python jargon) that often has more up-to-date packages and a broader selection than the default Anaconda repository. You should use for several reasons, but mostly because conda-forge generally has the largest volume of packages and the most up-to-date versions\nNote that some packages are only available through PyPi (pip). But you are covered for that: You can install pip packages within a conda env, by first activating the conda env and then normally using pip. pip should be part of your dependencies though. Always try to install packages using conda first.\nWe highly recommend using mamba as a drop-in replacement for conda, for much faster use.\nSome useful resources\n  A good resource for better understanding difference between mamba and conda, and their lightweights alternatives\n  Advanced tutorial on using conda environments …","date":1718064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719273600,"objectID":"90db55917be15cc7a3af16361adf8664","permalink":"https://vboussange.github.io/post/research-project-dependencies/","publishdate":"2024-06-11T00:00:00Z","relpermalink":"/post/research-project-dependencies/","section":"post","summary":"Your future self and others should be able to recreate the minimal environment to run the scripts in your research project. This is best achieved using **package managers**  and **virtual environments**.","tags":["julia","python"],"title":"A multi-language overview on how to handle dependencies within a research project","type":"post"},{"authors":["Mauro Werder","Victor Boussange"],"categories":null,"content":"I personally find that one of the biggest challenge when doing research is to keep things neat and organized. Having a good management system for your code and resources is key to optimizing time and brain resources. In this post, I discuss various methods for structuring a research project folder that includes code, data, publications, and more. Additionally, I discuss the specifics of organizing your research code. As I started my PhD, I wish I could have had some of such guidelines. But starting from scratch allowed me to build, with trials and errors, a good system for my later life. Hopefully, some of this can apply to you!\nThis post is part of a series of posts on best practices for managing research code. Much of this material was developed in collaboration with Mauro Werder as part of the Course On Reproducible Research, Data Pipelines, and Scientific Computing (CORDS). If you have experiences to share or spot any errors, please reach out!\nContent  Content Project folder structures code/ structure Turning your code/ into a “package” Wrapping up  Take-home messages    Project folder structures I quite like this project folder structure, which keeps apart raw data and results from the code, but still place them relatively close, together with admin and publications. Having a separate git repo for the paper is something I would recommend as well (possibly linked to an Overleaf project).\n|-- code/ |-- data/ |-- results |-- publications | |-- talks | |-- posters | |-- papers |-- admin |-- meetings |-- more-folders -- README.md You may want to place results within code, together with data (which you should not git track) The structure of code/ deserves here some attention.\ncode/ structure Programming languages typically have their own conventions, but often the folders follow this scheme\n a README.md file at the top level a src/ folder, containing models and other generic function and classes, that will be used in script/ files, example usages, e.g. examples/ scripts to run models, evaluation, etc., e.g. scripts/ documentation (often generated), e.g. docs/  It can make sense for research projects to distinguish between scripts placed in scripts/ and reused functions, models, etc., placed in src.\n Python Folder structure |-- src/ # package code |-- scripts/ # Custom analysis or processing scripts |-- tests/ |-- examples/ # Example scripts using the package |-- docs/ # documentation -- environment.yml # to handle project dependencies -- README.md   R Folder structure |-- R/ # R scripts and functions (package code) |-- scripts/ # Custom analysis or processing scripts |-- man/ # Documentation files |-- tests/ |-- examples/ # Example scripts using the package |-- vignettes/ # Long-form documentation -- DESCRIPTION # Package description and metadata -- NAMESPACE # Namespace file for package -- README.md # Project overview and details   Julia Folder structure |-- src/ # package code |-- scripts/ # Custom analysis or processing scripts |-- test/ |-- examples/ # Example scripts using the package |-- docs/ # documentation -- Project.toml # to handle project dependencies -- README.md  Turning your code/ into a “package” You may want to specify the src folder as a package. This has a few advantages, including\n not having to deal with relative position of files to call the functions in src/ maximizing your productivity by creating a generic package additionally to your main research project.  To import functions and classes (types) located in the src folder, you typically need to indicate in each script the relative path of src. In Julia, you would typically do something like include(\u0026#34;../src/path/to/your/src_file.jl\u0026#34;). In Python, you would do something like:\nimport sys sys.path.append(\u0026#34;../src/\u0026#34;) from src.path.to.your.src_file import my_fun If src/ directory grows, it’s beneficial to convert it into a separate package. Although this process is a bit more complex, it eliminates the need for path specifications, simplifies the import of functions and classes, and makes the codebase easily accessible for other research projects.\nThere are typically ways to turn a code-project into an installable package. This is in particular useful for code which other people (or yourself) use for different projects.\nYou can achieve this easily with development tools.\nFor Python, tools like setuptools and poetry facilitate package development. If you’re working in R, devtools is the go-to tool for developing packages. In Julia, the Pkg tool serves a similar purpose.\nPackage templates can be useful to simplify the creation of packages by generating package skeletons. In Python, checkout out cookiecutter. In R, check usethis. For Julia, use the Pkg.generate() built-in functionality, or the more advanced PkgTemplates.jl package.\nNote that you may want at some point to locate your src/ (and associated tests, docs, etc…) in a separate git repo.\nFurther reading for\n Python R Julia Pkg.jl documentation and how to create packages Modern Julia …","date":1718064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719273600,"objectID":"861d2bd77dee2dee90d003b49110ef93","permalink":"https://vboussange.github.io/post/best-practices-for-your-research-code/","publishdate":"2024-06-11T00:00:00Z","relpermalink":"/post/best-practices-for-your-research-code/","section":"post","summary":" In this post, we'll explore various methods for structuring a research project folder that includes code, data, publications, and more. Additionally, we'll look into the specifics of organizing your code folder.","tags":["julia","python"],"title":"A multi-language overview on how to organise your research project code and documents","type":"post"},{"authors":["Victor Boussange","Mauro Werder"],"categories":null,"content":"Code testing is essential to identify and fix potential issues, to maintain sanity over the course of the development of the project and quickly identify bugs, and to ensure the reliability and sanity of your experiment overtime.\nThis post is part of a series of posts on best practices for managing research project code. Much of this material was developed in collaboration with Mauro Werder as part of the Course On Reproducible Research, Data Pipelines, and Scientific Computing (CORDS). If you have experiences to share or spot any errors, please reach out!\nContent  Content Unit testing  Lightweight formal tests with assert  Python Julia   Testing with a test suite  Python Julia R   Testing non-pure functions and classes  Python     Continuous integration  Cool tip   Other types of tests Resources Take-home messages  Unit testing Unit testing involves testing a unit of code, typically a single function, to ensure its correctness. Here are some key aspects to consider:\n Test for correctness with typical inputs. Test edge cases. Test for errors with bad inputs.  Some developers start writing unit tests before writing the actual function, a practice known as Test-Driven Development (TDD). Define upstream on a piece of paper the behavior of the function, write corresponding tests, and when all tests pass, you are done. This philosophy ensures that you have a well-tested implementation, and avoids unnecessary feature development, forcing you to focus only on what is needed. While TDD is a powerful idea, it can be challenging to follow strictly.\nA good idea is to write an additional test when you find a bug in your code.\nLightweight formal tests with assert The simplest form of unit testing involves some sort of assert statement.\nPython def fib(x): if x \u0026lt;= 2: return 1 else: return fib(x - 1) + fib(x - 2) assert fib(0) == 0 assert fib(1) == 1 assert fib(2) == 1 Julia @assert 1 == 0 When one test is broken, you’ll get an error for the corresponding test, which you’ll need to fix to check the following tests.\nIn Julia or Python, you could directly place the assert statement after your functions. This way, tests are run each time you execute the script. Here is nother pythonic approach, which can be used to decouple the test\ndef fib(x): if x \u0026lt;= 2: return 1 else: return fib(x - 1) + fib(x - 2) if __name__ == \u0026#39;__main__\u0026#39;: assert fib(0) == 0 assert fib(1) == 1 assert fib(2) == 1 assert fib(6) == 8 assert fib(40) == 102334155 print(\u0026#34;Tests passed\u0026#34;) Consider using np.isclose, np.testing.assert_allclose (Python) or approx (Julia) for floating point comparisons.\nTesting with a test suite Once you have many tests, it makes sense to group them into a test suite and run them with a test runner. This approach will run all tests, even though some are broken, and retrieve and informative statements on those tests that passed, and those that did not. As you’ll see, it also allows to automatically run the test at each commit, with continuous integration.\nPython Two main frameworks for unit tests in Python are pytest and unittest, with pytest being more popular.\nExample using pytest:\nfrom src.fib import fib import pytest def test_typical(): assert fib(1) == 1 assert fib(2) == 1 assert fib(6) == 8 assert fib(40) == 102334155 def test_edge_case(): assert fib(0) == 0 def test_raises(): with pytest.raises(NotImplementedError): fib(-1) with pytest.raises(NotImplementedError): fib(1.5) Run the tests with:\npytest test_fib.py Julia Built in module Test, relying on the macro @test. Consider grouping your tests with\njulia\u0026gt; @testset \u0026#34;trigonometric identities\u0026#34; begin θ = 2/3*π @test sin(-θ) ≈ -sin(θ) @test cos(-θ) ≈ cos(θ) @test sin(2θ) ≈ 2*sin(θ)*cos(θ) @test cos(2θ) ≈ cos(θ)^2 - sin(θ)^2 end; This will nicely output\nTest Summary: | Pass Total Time trigonometric identities | 4 4 0.2s which comes handy for grouping tests applied to a single function or concept. Test functions may require additional packages to your minimum working environment specified at your package root folder. An additional virtual environment may be specified for tests! To develop my tests interactively, I like using TestEnv. Unfortunately, using Pkg.activate in tests would not work there, you. You need TestEnv to have access to your package functions;\nIn your package environment,\nusing TestEnv TestEnv.activate() will activate the test environment.\nTo reactivate the normal environment,\nPkg.activate(\u0026#34;.\u0026#34;) Here is a nice thread to read more on that.\nR testhat\nTesting non-pure functions and classes For nondeterministic functions, provide the random seed or variables needed by the function as arguments to make them deterministic. For stateful functions, test postconditions to ensure the internal state changes as expected. For functions with I/O side effects, create mock files to verify proper input reading and expected output.\nPython def file_to_upper(in_file, out_file): fout = open(out_file, \u0026#39;w\u0026#39;) with open(in_file, \u0026#39;r\u0026#39;) as f: for line in f: fout.write(line.upper()) fout.close() …","date":1718064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719273600,"objectID":"d166ecdb9e9a858e8b2a05ade401aa43","permalink":"https://vboussange.github.io/post/testing-your-research-code/","publishdate":"2024-06-11T00:00:00Z","relpermalink":"/post/testing-your-research-code/","section":"post","summary":"Code testing is essential to identify and fix potential issues, to maintain sanity over the course of the development of the project and quickly identify bugs, and to ensure the reliability and sanity of your experiment overtime.","tags":["julia","python"],"title":"A multi-language overview on how to test your research project code","type":"post"},{"authors":["Alexander Skeels","Lydian Boschman","Ian R. McFadden","Elizabeth Marie Joyce","Oskar Hagen","Octavio Jiménez Robles","Wilhelmine Bach","Victor Boussange","Thomas Keggin","Walter Jetz","Loïc Pellissier"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ","date":1688688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688688000,"objectID":"96d2ceb35ca6051a8c133e2caaaefc21","permalink":"https://vboussange.github.io/publication/dream/","publishdate":"2023-07-07T00:00:00Z","relpermalink":"/publication/dream/","section":"publication","summary":"Faunal turnover in Indo-Australia across Wallace’s Line is one of the most recognizable patterns in biogeography and has catalyzed debate about the role of evolutionary and geoclimatic history in biotic interchanges. Here, analysis of more than 20,000 vertebrate species with a model of geoclimate and biological diversification shows that broad precipitation tolerance and dispersal ability were key for exchange across the deep-time precipitation gradient spanning the region. Sundanian (Southeast Asian) lineages evolved in a climate similar to the humid “stepping stones” of Wallacea, facilitating colonization of the Sahulian (Australian) continental shelf. By contrast, Sahulian lineages predominantly evolved in drier conditions, hampering establishment in Sunda and shaping faunal distinctiveness. We demonstrate how the history of adaptation to past environmental conditions shapes asymmetrical colonization and global biogeographic structure.","tags":["Biodiversity","Biogeography","Evolutionary ecology"],"title":"Paleoenvironments shaped the exchange of terrestrial vertebrates across Wallace’s Line","type":"publication"},{"authors":["Victor Boussange"],"categories":["Sailing tutorials"],"content":"Back in the days, people used to orient themselves with paper maps and the stars or with a sextant. Unfortunately, we have lost this knowledge. It would now be difficult, at least for me, to live without a digital map and a GPS. In this blog post, I detail how to set up a handy navigation system for sailing, using a Raspberry Pi and OpenPlotter to transmit GPS and AIS signals over Wifi to the Navionics Boating app.\nChoosing the Right Chart Maps There are plenty of digital chart map options available for navigation, and we had to figure out which one would fit our need on the boat. After careful consideration, we narrowed down our choices to two alternatives: o-charts charts, to be used in combination with OpenCPN, an open source navigation software, or Navionics charts, to be used with the “Navionics Boating” application. Both had a similar pricing for what we wanted (charts for Germany, Denmark, Sweden, Norway, Shetland Islands, UK, Ireland and France): around 120-150EUR, with a slight advantage for Navionics. Pros for o-charts is that you can use them more than one year, although the update option is only valid for a year (I think, although I am not 100% sure). Navionics charts is only valid for a year, the period of the subscription. Pros for Navionics is that you can use your subscription on many devices (at least 5, I think), while you cannot use o-charts on iOS devices and on more than 2 devices. Because we wanted to have charts on our smartphones, we decided to go for Navionics.\nThe Importance of Accurate GPS and AIS While our smartphones’ GPS serves us well in our daily lives, accurate positioning becomes critical when sailing. It ensures that we navigate safely, avoiding shallow waters and potential collisions. Additionally, during nighttime navigation, an instrument called AIS (Automatic Identification System) proves invaluable by providing information about nearby large ships.\nIn the following, I explain how I installed a server on our boat that transmits GPS and AIS to the “Navionics Boating” application our smartphone and tablets through Wifi. For this, I used a Raspberry Pi 4, OpenPlotter, and a GPS beacon and a radio antenna fixed on the outside of the boat and connected to the Raspberry.\nInstalling OpenPlotter on the Raspberry Pi To get started, you need to install OpenPlotter, a Linux distribution designed for Raspberry Pi and that contains the essential softwares for navigation. I used the 64-bit OpenPlotter Starting image that contains an appropriate pre-built kernel. To install it, you’ll need a less than 32GB SD-card, to be formatted in FAT32. My problem was that the SD card I had had already been used on a Raspberry Pi, and as such contained an EXT4 partition. EXT4 partitions are used by Linux systems, but are not recognized by MacOS. This prevented me to format the card in FAT32. To allow formatting, I used the diskutil utility from MacOS.\nFirst run\ndiskutil list\nin the Terminal. This allows you to identify your SD card, in my case /dev/disk4. I had previously installed ext4fuse, not sure if this is a required step. To format the SD card, execute the command\nsudo diskutil eraseDisk FAT32 RASPBERRY MBRFormat /dev/disk4 You can change RASPBERRY with any name you like best. More details of this command in this thread.\nNow the SD card is ready to be used. Download the Raspberry Pi Imager to install the image on the SD card. You first need to unzip the image, then execute Raspberry Pi Imager, click “Choose OS” and click on “Use custom”. Locate the .img file, then choose the SD card in “Choose storage” and hit “Write”. The SD card is ready. Insert it in the Raspberry Pi and swith the power on. The system is going boot on the SD card and set up OpenPlotter. This took a relatively short amount of time.\nSetting up OpenPlotter I had an external monitor that I could use for the Raspberry Pi. I encountered a minor hurdle as OpenPlotter failed to identify it correctly, and did not properly display. I had to adjust the resolution of the screen by hitting the raspberry, then “Preferences” and “Screen configuration”. Check also the configuration of the monitor, that may be set to zoom in the visual signal.\nSetting up the GPS Configuring the GPS was a straightforward process that involved following this tutorial.\nSetting up the AIS Similar to configuring the GPS, we followed this video tutorial (in French, with translation available) to set up the AIS system. At first, I did not correctly calibrate the PPM offset, and although I did follow the rest of the procedure correctly, I had an “inactive” AIS process. I then redid the whole procedure, properly waiting for more than an hour to get the PPM initial guess correct, and with this value, I did manage to make it work.\nGetting the signal on Navionics through OpenPlotter Access Point To transmit the GPS and AIS data to the Navionics application, we established an access point using the Raspberry Pi. This allowed us to create a Wi-Fi network, enabling our …","date":1685059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685059200,"objectID":"07bbe11ed38d28d0cc7b9926dbaaee7e","permalink":"https://vboussange.github.io/post/navigationsystem/","publishdate":"2023-05-26T00:00:00Z","relpermalink":"/post/navigationsystem/","section":"post","summary":"Back in the days, people used to orient themselves with paper maps and the stars or with a sextant. Unfortunately, we have lost this knowledge. It would now be difficult, at least for me, to live without a digital map and a GPS.","tags":["Sailing","Open Source Project"],"title":"Open source navigation system for sailing","type":"post"},{"authors":["Victor Boussange"],"categories":null,"content":"In this post, I explore the benefits and drawbacks of using empirical (ML)-based models versus mechanistic models for predicting ecosystem responses to perturbations. To evaluate these different modelling approaches, I use a mechanistic ecosystem model to generate a synthetic time series dataset.\nBy applying both modelling approaches to this dataset, I can evaluate their performance. While the ML-based approach yields accurate forecasts under unperturbed dynamics, it inevitably fails when it comes to predicting ecosystem response to perturbations. On the other hand, the mechanistic model, which is simplified version of the ground truth model to reflect a realistic scenario, is inaccurate and cannot forecast, but provides a more adequate approach to predict ecosystem response to unobserved scenarios.\nTo improve the accuracy of mechanistic models, I introduce inverse modelling, and in particular an approach that I have developed called piecewise inference. This approach allows to accurately calibrate complex mechanistic models, and doing so open doors to improve our understanding ecosystems by performing model selection.\nFinally, I discuss how hybrid models, which incorporate both ML-based and mechanistic components, offer the potential to benefit from the strengths of both modelling approaches. By examining the strengths and limitations of these different modelling approaches, I hope to provide insights into how best to use them to advance our knowledge of ecological and evolutionary dynamics.\nNotes  This post is under construction, and contain typos! If you find some, please contact me so that I can correct For the sake of clarity, some pieces of code have voluntarily been hidden in external Julia files, which are loaded throughout the post. If you want to inspect them, check out those files in the corresponding GitHub repository  Generating a synthetic dataset To generate the synthetic dataset, I consider a 3 species ecosystem model, composed of a resource, consumer and prey species. The resource growth rate depends on water availability. Here is a simplified version of the dynamics\n$$\\begin{aligned} \\text{basal growth of } \\text{🌱} \u0026amp;= f(\\text{💧})\\\\ \\text{per capita growth rate }\\text{🌱} \u0026amp;= \\text{basal growth} - \\text{competition} - \\text{grazing} - \\text{death}\\\\ \\text{per capita growth rate }\\text{🦓} \u0026amp;= \\text{grazing} - \\text{predation} - \\text{death}\\\\ \\text{per capita growth rate }\\text{🦁} \u0026amp;= \\text{predation} - \\text{death} \\end{aligned}$$\nLet’s implement that in Julia!\ncd(@__DIR__) import Pkg; Pkg.activate(\u0026#34;.\u0026#34;) using PythonCall nx = pyimport(\u0026#34;networkx\u0026#34;) np = pyimport(\u0026#34;numpy\u0026#34;) include(\u0026#34;model.jl\u0026#34;) include(\u0026#34;utils.jl\u0026#34;); To implement the model, I use the library EcoEvoModelZoo, which provides to the user ready-to-use mechanistic eco-evolutionary models. I use the model type SimpleEcosystemModel, cf. documentation of EcoEvoModelZoo. Let’s first construct the trophic network, and plot it\nusing EcoEvoModelZoo N = 3 # number of species pos = Dict(1 =\u0026gt; [0, 0], 2 =\u0026gt; [0.2, 1], 3 =\u0026gt; [0, 2]) labs = Dict(1 =\u0026gt; \u0026#34;Resource\u0026#34;, 2 =\u0026gt; \u0026#34;Consumer\u0026#34;, 3 =\u0026gt; \u0026#34;Prey\u0026#34;) foodweb = DiGraph(N) add_edge!(foodweb, 2 =\u0026gt; 1) # Consumer (node 2) feeds on Resource (node 1) add_edge!(foodweb, 3 =\u0026gt; 2) # Predator (node 3) fonds on consumer (node 2) println(foodweb) Graphs.SimpleGraphs.SimpleDiGraph{Int64}(2, [Int64[], [1], [2]], [[2], [3], Int64[]]) plot_foodweb(foodweb, pos, labs) (\u0026lt;py Figure size 640x480 with 1 Axes\u0026gt;, \u0026lt;py Axes: \u0026gt;)    Then, I implement the processes that drive the dynamics of the ecoystem. Those include resource limitation for the resource species (e.g. limitation in nutrients), intraspecific competition for the resource species, reproduction, and feeding interactions (grazing and predation). To better understand this piece of code, you may want to refer to one of my previous blog post, “Inverse ecosystem modelling made easy with PiecewiseInference.jl”.\nfunction carrying_capacity(p, t) @unpack K₁₁ = p K = vcat(K₁₁, ones(Float32, N - 1)) return K end carrying_capacity (generic function with 1 method) function competition(u, p, t) @unpack A₁₁ = p A = spdiagm(vcat(A₁₁, zeros(Float32, 2))) return A * u end competition (generic function with 1 method) resource_conversion_efficiency(p, t) = ones(Float32, N) resource_conversion_efficiency (generic function with 1 method) Functional responses The feeding processes implemented are based on a functional response of type II. The attack rates q define the slope of the functional response, while the handling times H define the saturation of this response.\nW = adjacency_matrix(foodweb) I, J, _ = findnz(W) function feeding(u, p, t) @unpack H₂₁, H₃₂, q₂₁, q₃₂ = p # handling time H = sparse(I, J, vcat(H₂₁, H₃₂), N, N) # attack rates q = sparse(I, J, vcat(q₂₁, q₃₂), N, N) return q .* W ./ (one(eltype(u)) .+ q .* H .* (W * u)) end feeding (generic function with 1 method) Dependence of resource growth rate on water availability We model a time-varying water availability, and a growth rate of the …","date":1680220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680220800,"objectID":"83c6968528d073ad6e1dfa2201816da4","permalink":"https://vboussange.github.io/post/hybridmodelling/","publishdate":"2023-03-31T00:00:00Z","relpermalink":"/post/hybridmodelling/","section":"post","summary":"In this post, I explore the benefits and drawbacks of using empirical (ML)-based models versus mechanistic models for predicting ecosystem responses to perturbations, and further develeop a hybrid approach combining their strengths.","tags":["Ecology and evolution","Julia","Scientific Machine Learning"],"title":"On combining machine learning-based and theoretical ecosystem models","type":"post"},{"authors":[],"categories":[],"content":"Mechanistic ecosystem models permit to quantiatively describe how population, species or communities grow, interact and evolve. Yet calibrating them to fit real-world data is a daunting task. That’s why I’m excited to introduce PiecewiseInference.jl, a new Julia package that provides a user-friendly and efficient framework for inverse ecosystem modeling. In this blog post, I will guide you through the main features of PiecewiseInference.jl and provide a step-by-step tutorial on how to use it with a three-compartment ecosystem model. Whether you’re a quantitative ecologist or a curious data scientist, I hope this post will encourage you to join the effort and use and develop inverse ecosystem modelling methods to improve our understanding and predictions of ecosystems.\nPreliminary steps This tutorial relies on three packages that I have authored but are (yet) not registered on the official Julia registry. Those are\n PiecewiseInference, EcoEvoModelZoo: a package which provides access to a collection of ecosystem models, ParametricModels: a wrapper package to manipulate dynamical models. Specifically, ParametricModels avoids the hassle of specifying, at each time you want to simulate an ODE model, boring details such as the algorithm to solve it, the time span, etc…  To easily install them on your machine, you’ll have to add my personal registry by doing the following:\nusing Pkg; Pkg.Registry.add(RegistrySpec(url = \u0026#34;https://github.com/vboussange/VBoussangeRegistry.git\u0026#34;)) Once this is done, let’s import those together with other necessary Julia packages for this tutorial.\nusing Graphs using EcoEvoModelZoo using ParametricModels using LinearAlgebra using UnPack using OrdinaryDiffEq using Statistics using SparseArrays using ComponentArrays using PythonPlot We use Graphs to create a directed graph to represent the food web to be considered The OrdinaryDiffEq package provides tools for solving ordinary differential equations, while the LinearAlgebra package is used for linear algebraic computations. The UnPack package provides a convenient way to extract fields from structures, and the ComponentArrays package is used to store and manipulate the model parameters conveniently. Finally, the PythonCall package is used to interface with Python’s Matplotlib library for visualization.\nDefinition of the forward model Defining hyperparameters for the forward simulation of the model. Next, we define the algorithm used for solving the ODE model. We also define the absolute tolerance (abstol) and relative tolerance (reltol) for the solver. tspan is a tuple representing the time range we will simulate the system for, and tsteps is a vector representing the times we want to output the simulated data.\nalg = BS3() abstol = 1e-6 reltol = 1e-6 tspan = (0.0, 600) tsteps = range(300, tspan[end], length=100) 300.0:3.0303030303030303:600.0 Defining the foodweb structure We’ll define a 3-compartment ecosystem as presented in McCann et al. (1994). We will use SimpleEcosystemModel from EcoEvoModeZoo.jl, which requires as input a foodweb structure. Let’s use a DiGraph to represent it.\nN = 3 # number of compartment foodweb = DiGraph(N) add_edge!(foodweb, 2 =\u0026gt; 1) # C to R add_edge!(foodweb, 3 =\u0026gt; 2) # P to C true The N variable specifies the number of compartments in the model. The add_edge! function is used to add edges to the graph, specifying the flow of resources between compartments.\nFor fun, let’s just plot the foodweb. Here we use the PythonCall and PythonPlot packages to visualize the food web as a directed graph using networkx and numpy. We create a color list for the different species, and then create a directed graph g_nx with networkx using the adjacency matrix of the food web. We also specify the position of each node in the graph, and use nx.draw to draw the graph with\nusing PythonCall nx = pyimport(\u0026#34;networkx\u0026#34;) np = pyimport(\u0026#34;numpy\u0026#34;) species_colors = [\u0026#34;tab:red\u0026#34;, \u0026#34;tab:green\u0026#34;, \u0026#34;tab:blue\u0026#34;] g_nx = nx.DiGraph(np.array(adjacency_matrix(foodweb))) pos = Dict(0 =\u0026gt; [0, 0], 1 =\u0026gt; [0.2, 1], 2 =\u0026gt; [0, 2]) labs = Dict(0 =\u0026gt; \u0026#34;Resource\u0026#34;, 1 =\u0026gt; \u0026#34;Consumer\u0026#34;, 2 =\u0026gt; \u0026#34;Prey\u0026#34;) fig, ax = subplots(1) nx.draw(g_nx, pos, ax=ax, node_color=species_colors, node_size=1000, labels=labs) display(fig)    Defining the ecosystem model Now that we have defined the foodweb structure, we can build the ecosystem model, which will be a SimpleEcosystemModel from EcoEvoModelZoo.\nThe next several functions are required by SimpleEcosystemModel and define the specific dynamics of the model. The intinsic_growth_rate function specifies the intrinsic growth rate of each compartment, while the carrying_capacity function specifies the carrying capacity of each compartment. The competition function specifies the competition between and within compartments, while the resource_conversion_efficiency function specifies the efficiency with which resources are converted into consumer biomass. The feeding function specifies the feeding interactions between compartments. …","date":1679788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679788800,"objectID":"c4307fabcccfc98c6d6b74c8cf9f231b","permalink":"https://vboussange.github.io/post/piecewiseinference/","publishdate":"2023-03-26T00:00:00Z","relpermalink":"/post/piecewiseinference/","section":"post","summary":"This blog post discusses the use of PiecewiseInference.jl, a Julia package that enables the use of machine learning to fit complex ecological models on ecological dataset.","tags":["Scientific Machine Learning","Julia"],"title":"Inverse ecosystem modeling made easy with PiecewiseInference.jl","type":"post"},{"authors":["Victor Boussange","Didier Sornette","Heike Lischke","Loïc Pellissier"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ","date":1674518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674518400,"objectID":"24ac9bce13c729e563369ca251ba58d7","permalink":"https://vboussange.github.io/publication/econobiology/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/econobiology/","section":"publication","summary":"Analogies between organisational routines and genes, firms and phenotypes, economic activities and biological populations, and between the processes acting upon these entities, have been used to advance our qualitative understanding of mechanisms driving economic change. Yet, it remained unclear whether biological concepts can be used to quantitatively describe long-term economic change. Here, we use an inverse modelling framework together with economic time-series data to test whether eco-evolutionary processes can explain the collective dynamics of national economic activities. Comparing the support of different biologically inspired dynamic community models against a null model, we find evidence for positive interactions between economic activities and their spatial dispersal.","tags":["Scientific machine learning","Inverse modelling","Economic complexity","Eco-evolutionary dynamics","Food-webs"],"title":"Processes analogous to ecological interactions and dispersal shape the dynamics of economic activities","type":"publication"},{"authors":[],"categories":[],"content":"Introduction In this tutorial, we’ll learn the basics of approximate Bayesian computation (ABC). ABC is a very flexible inference method that is quite intuitive, and that you can apply to almost any model type. As such, it is quite handy to have it in one’s toolbox! To build up an intuition of the method, we’ll perform ABC on a very simple ecosystem model, using Julia, and will visualize graphically the inference results.\nPackage loading We first need to load a few packages.\ncd(@__DIR__) using Pkg; Pkg.activate(\u0026#34;.\u0026#34;) using Random using LinearAlgebra import Optimisers:destructure # required to simplify parameter indexing using UnPack # for parameter indexing using ApproxBayes # ABC toolbox using Distributions # useful to define the priors using OrdinaryDiffEq # ODE solver library using ParametricModels # convenience package for ODE models. /!\\ package not registered It is always important to make your work reproducible, so we’ll set manually a seed to the random number generator.\n# Set a seed for reproducibility. Random.seed!(11); Model definition and data generation What we’ll do is to implement an Ordinary Differential Equation model, which will be used to generate synthetic data and further perform the inference. The idea here is to consider this synthetic data as our empirical data, and pretend that we do not know which parameters generated this data. Our goal is then to recover the generating parameters.\nWe’ll consider a predator-prey ecological model, the Lotka-Volterra equations. This model has a total of four different parameters $\\alpha, \\beta, \\gamma, \\delta$, that describe the interactions between the two species. For the sake of simplicity, we’ll assume that we know perfectly the parameters $\\gamma, \\delta$, and seek to infer $\\alpha$ and $\\beta$. Assuming to know $\\gamma, \\delta$ is of course a very unrealistic assumption… but a handy assumption to develop an intuition about parameter inference.\nWe use ParametricModels.jl to define our model. ParametricModels.jl is a simple wrapper around OrdinaryDiffEq.jl, that allows to play around with the model without bothering further about the details of the numerical solve. This makes it easier to simulate repeatedly an ODE model. For readability and maintenance, we’ll encapsulate the parameters in a NamedTuple.\n# Declaration of the model ParametricModels.@model LotkaVolterra # Definition of the model function (lv::LotkaVolterra)(du, u, p, t) # Model parameters @unpack α, β, = p # Current state x, y = u # fixed parameters γ = 3.0 δ = 1.0 # Evaluate differential equations du[1] = (α[] - β[] * y) * x # prey du[2] = (δ * x - γ) * y # predator return nothing end # Define initial-value problem. u0 = [1.0, 1.0] p = (α=[1.5], β=[1.0]) tspan = (0.0, 10.0) model = LotkaVolterra(ModelParams(;u0, p, tspan, alg=Tsit5(), saveat=0.1)) sol = simulate(model); And here we go, we have our model defined!\n As an exercise, you could try to write the same model with OrdinaryDiffEq.jl.\n Let’s now plot the model output. We first need to import some plotting utilities. For plotting, I tend to use PyPlot.jl, which is a wrapper around the Python library matplotlib. I find it much more convenient than the Julia package Plots.jl, in the sense that it contains more utility functions. It is always a good idea to load in parallel PyCall.jl, which allows to import other utility funtions from Python.\nusing PyPlot # to plot 3d landscape using PyCall Here is a plot of the model output without noise:\nfig = PyPlot.figure() PyPlot.plot(sol\u0026#39;) display(fig)    We now use sol to generate synthetic data containing some noise (in mathematical terms, we call this a Gaussian white noise, which follows $\\mathcal{N}(0,\\sigma)$ where $\\sigma = 0.8$.\nσ = 0.8 odedata = Array(sol) + σ * randn(size(Array(sol))) fig = figure() PyPlot.plot(odedata\u0026#39;) display(fig)    The model and the synthetic data are ready, let’s get started with ABC!\nApproximate Bayesian computation For ABC, one needs to define a function $\\rho$ that measures the distance between the model output $\\hat \\mu$ (in the picture below, $\\mu_i$) and the empirical data $\\mu$.\n   In our case, the empirical data available (odedata) allows us to explicitly define the likelihood of the model given the data. As a distance function, we therefore can first use the negative log of the likelihood. For additive Gaussian noise, the likelihood of the model is given by\n$$\\begin{split} p(y_{1:K} | \\theta, \\mathcal{M}) \u0026amp;= \\prod_{i=1}^K p(y_{i} | \\theta, \\mathcal{M})\\\\ \u0026amp;= \\prod_{k=1}^K \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma_y|}} \\exp \\left(-\\frac{1}{2} \\epsilon_k^{T} \\Sigma_y^{-1} \\epsilon_k \\right) \\end{split}$$\nwhere $\\epsilon_k \\equiv \\epsilon(t_k) = y(t_k) - h\\left(\\mathcal{M}(t_k, \\theta)\\right)$ and $\\Sigma_y = \\sigma^2 I$ in our case.\nSo let’s translate all this in Julia code.\nApproxBayes.jl requires as input a function simfunc(params, constants, data), which plays the role of the $\\rho$ function. It should return the distance value, as well as a second value that …","date":1669507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669507200,"objectID":"827bc8ba64f84c1710ee9a2013ac1221","permalink":"https://vboussange.github.io/post/abc_inference/","publishdate":"2022-11-27T00:00:00Z","relpermalink":"/post/abc_inference/","section":"post","summary":"In this tutorial, you'll learn the basics of approximate Bayesian computation (ABC). ABC is an inference method with very little requirements in terms of the model structure - yet it can be very powerful. It is very simple to apply to any model, and to understand. We'll play around with Julia, and we will visualize graphically the inference results, so that you can build an intuition of the inference method.","tags":[],"title":"A practical introduction to approximate Bayesian computation","type":"post"},{"authors":["Victor Boussange"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ","date":1664928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664928000,"objectID":"422b01dbe09bdfe1673aa0f5029d9baa","permalink":"https://vboussange.github.io/publication/phd_thesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/phd_thesis/","section":"publication","summary":"Ecological and economic systems are complex adaptive systems, composed of multiple entities which interact and undergo evolutionary processes. The complexity of these systems present a crucial challenge for underpinning their general organizational principles. This thesis develops innovative modelling techniques and advances our comprehension of the eco-evolutionary processes and feedbacks that shape these systems.","tags":["Deep Learning","eco-evolutionary dynamics","economic system","inverse modelling","Scientific Machine Learning","PDEs"],"title":"Forward and inverse modelling of eco-evolutionary dynamics in ecological and economic systems","type":"publication"},{"authors":["Victor Boussange","Pau Vilimelis Aceituno","Loïc Pellissier"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ","date":1658793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658793600,"objectID":"c4600c3088651395e274eb4e30a1fb3e","permalink":"https://vboussange.github.io/publication/mini-batching/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mini-batching/","section":"publication","summary":"Ecosystems are involved in global biogeochemical cycles that regulate climate and provide essential services to human societies. Mechanistic models are required to describe ecosystem dynamics and anticipate their response to anthropogenic pressure, but their adoption has been limited in practice because of issues with parameter identification and because of model inaccuracies. We propose a machine learning (ML) framework relying on a mini-batch method combined with automatic differentiation and state-of-the-art optimizers. The proposed ML framework can efficiently learn from data and elucidate mechanistic pathways to improve our understanding and predictions of ecosystem dynamics.","tags":["Scientific machine learning","Inverse modelling","Ecosystem modelling","Model selection","Food-webs"],"title":"Mini-batching ecological data to improve ecosystem models with machine learning","type":"publication"},{"authors":["Victor Boussange","Loïc Pellissier"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ","date":1657065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657065600,"objectID":"338262bb5c599bf15ae6d891b2f0f311","permalink":"https://vboussange.github.io/publication/differentiation-in-graphs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/differentiation-in-graphs/","section":"publication","summary":"Differentiation mechanisms are influenced by the properties of the landscape over which individuals interact, disperse and evolve. Here, we investigate how habitat connectivity and habitat heterogeneity affect phenotypic differentiation by formulating a stochastic eco-evolutionary model where individuals are structured over a spatial graph. By formalising the eco-evolutionary and spatial dynamics of biological populations on graphs, our study establishes fundamental links between landscape features and phenotypic differentiation.","tags":["Biodiversity","Evolutionary ecology","Network topology","Population dynamics"],"title":"Eco-evolutionary model on spatial graphs reveals how habitat structure affects phenotypic differentiation","type":"publication"},{"authors":["Victor Boussange","Sebastian Becker","Arnulf Jentzen","Benno Kuckuck","Loïc Pellissier"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). -- ","date":1651708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651708800,"objectID":"c7cd4a4622bb52609a78e87d7300773e","permalink":"https://vboussange.github.io/publication/deep_learning_for_pdes/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/deep_learning_for_pdes/","section":"publication","summary":"Nonlinear partial differential equations (PDEs) are used to model dynamical processes in a large number of scientific fields, ranging from finance to biology. In this article we propose two numerical methods based on machine learning and on Picard iterations, respectively, to approximately solve non-local nonlinear PDEs. Our work extends recently developed methods to overcome the curse of dimensionality in solving PDEs.","tags":["Deep Learning","Scientific Machine Learning","PDEs"],"title":"Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions","type":"publication"},{"authors":[],"categories":[],"content":"This website is based on the wowchemy template that relies on Hugo, a fast framework for building static websites. Although Hugo has many advantages over its brother Jekyll, a Hugo website is not as easy to deploy on GitHub Pages, the free service offered by GitHub to host a personal website. If you want to build your Hugo website and deploy it easily, here is the recipe.\nBuild your Hugo website  Fork the github repo of the wowchemy template. Rename the repo as your-username.github.io Clone the repo and customise the template following your own taste, following the wowchemy tutorial. You can locally build the website with the command hugo server and access it at the adress indicated in the the text printed after the execution of the command. Once you are satified with your local website, it is time to publish it! I found out that the easiest way to do so is to create a GitHub Action.  Deploy automatically a Hugo website on GitHub pages Here I detail how to set up a workflow that will build your website and publish it at each new commit. The idea is to set up a GitHub Action, which job will be to\n Clone the master from your repo Build the Hugo website Create / clone a gh-pages branch Copy the static files generated in 2. to the gh-pages repo Push those changes to the gh-pages.  To do so, first create a file .github/workflows/gh-pages.yml and add the following content, replacing your-username by your … username.\nname: Build and Deploy on: push: branches: - master jobs: build: runs-on: ubuntu-latest steps: - name: Checkout master uses: actions/checkout@v1 with: submodules: true - name: Hugo Deploy GitHub Pages uses: benmatselby/hugo-deploy-gh-pages@master env: GO_VERSION: 1.17 HUGO_VERSION: 0.95.0 HUGO_EXTENDED: true TARGET_REPO: your-username/your-username.github.io TARGET_BRANCH: gh-pages TOKEN: ${{ secrets.TOKEN_HUGO_DEPLOY }} CNAME: vboussange.github.io You then need to generate a personnal access token. Here is a tutorial to do so. Tick the box “repo”, to grant full control of private repo. More details on why you need to do so are given in Jame Wright post (see below). Copy the generated code, and go in the settings of your “your-username.github.io” repo. There, create a secret, call it TOKEN_HUGO_DEPLOY and paste the previously generated token.\nYou are almost all set! Make your first commit. Wait for the action to execute. Once you see the green badge symbolising the success of the deployment Action, go to the Settings of your repo, and in “Pages” in the left side bar, under “Source” select the branch gh-pages. After a few minutes, your website should be available at https://your-username.github.io/.\nEnjoy!\n This post was greatly inspired by James Wright blog post on the same topic, although his Github Action was not quite working for me.\n ","date":1648395615,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648395615,"objectID":"8df1109087b723c43e4ec84a29ba363b","permalink":"https://vboussange.github.io/post/create-your-hugo-website/","publishdate":"2022-03-27T17:40:15+02:00","relpermalink":"/post/create-your-hugo-website/","section":"post","summary":"This website is based on the wowchemy template that relies on Hugo, a fast framework for building static websites. Although Hugo has many advantages over its brother Jekyll, a Hugo website is not as easy to deploy on GitHub Pages, the free service offered by GitHub to host a personal website.","tags":[],"title":"Create and deploy your Hugo website","type":"post"},{"authors":[],"categories":[],"content":"Sailing  Bought Tippelei, a 40 year old, 36 feet sailing boat, and sailed it to Norway from May to September 2023 during a sabbatical.   Ski mountaineering / Backcountry skiing   Wintertürmli 3002m, from Meiental, AD-, Switzerland, January 2025\n  Plattenburg 2747m, face W, 45°, Switzerland, January 2025\n   Dent d’Hérens 4171m, face SW arrête W, AD, Switzerland, May 2024               View this post on Instagram                      Finsteraarhorn 4274m, AD, Switzerland, April 2024 Wyssnollen 3590m, from Grünhornlücke, AD, Switzerland, April 2024     From Tromsø to Norkapp with bicycle and skis, Norway, March 2024\n 🚲 Tromsø - Laksvelbukt Tomas couloir Banana couloir of Holmbukktinden, 45˚ 🚲 Laksvelbukt - Nordkjosbotn Lille Russetinden 1527m 🚲 Nordkjosbotn - Lyngseidet New Year’s Rocket, 50˚ Istinden 1490m Store Kjsotinden 1488m 🚲 Lyngseidet - Skervøy Store Kågtinden 1228m 🚲 Skervøy - Nordkapp      Druesberg 2282m, D-, from Zürich with bicycle, Switzerland, February 2024    Piz Badus 2928m, E face, PD+, Switzerland, January 2024 Piz Ravetsch 3007m, NW face, PD, Switzerland, January 2024 Aiguille de l’Épaisseur 3230m, SE face, PD, France, January 2024 Pizzo Rotondo 3192m, from Capanna Piansecco, D+, Switzerland, January 2024 Râteau d’Aussois 3131m, NW face, PD+, France, January 2024 Winterhorn / Pizzo d’Orsino 2662m, NW face, AD+, Switzerland, December 2023 Chrüz 2195m, F, Switzerland, December 2023 Girenspitz 2367m, PD+, Switzerland, December 2023 Piet 1965m, PD+, Switzerland, December 2023 Mont Blanc 4808m, uphill via Arrête Nord and descent via Grand and Petit Plateau, 3.2 AD, France, May 2023 Bishorn 4153m, from cabane de Tracuit, PD, Switzerland, April 2023 Mt Vélan 3727m, uphill and downhill via Couloir Hannibal, 4.3 D, Switzerland Traverse of the Alps from Innsbruck to Venezia with bicycles and skis, 11 days, Mars 2023  🚲 Innsbruck (Austria) - Valles (Italy) Wilde Kreuzspitze 3132m 🚲 Valles - Lungiarù Ciampani 2668m, via Canale Norte 🚲 Lungiarù - Misurina Canale Norte “Staunies” Forcella degli Angeli - Forcella del Diavolo 🚲 Misurina - Mareson Pecol Mt Pelmo 3168m 🚲 Mareson Pecol - Venezia       Trou de la Mouche 2453m, Tour du Passage du Père, 45° AD, Aravis, France, 2023\n  Pointe du Midi 2364m, Aravis, France, 2023\n  Fuggstock 2371m, PD, Glarus, Switzerland, 2023\n  Rote Totz 2847m, PD, Wallis, Switzerland, 2023\n  Horlini 2458m, Wallis, Switzerland, 2023\n  Pic de l’Étendard 3464m, PD, 2022\n  Roche du Chardonnet 2950m, PD, 2022\n  Haute Route Chamonix - Zermatt, 7 days, group leader, February 2022 - video 📺\n Grande Tête de By 3588m Tête de Valpelline 3798m    Redertenstock 2214m, 2022, PD\n  Oberalpstock 3328m, über den Staldenfirn ins Maderanertal, D, 2022\n  Chronenstock 2451m, AD-, 2022\n  Spilauer Grätli 2303m, PD, 2022\n  Hundstock 2212m, AD-, 2022\n  Hagelstock 2181m, L, 2022\n  Pic du Rognolet 2659m, PD, 2021\n  Petit Chateau 2458m, Couloir NE, 40° / PD-, 2021\n  Chläbdächer 2017m, PD+, 2021\n  Strahlhorn 4190m, from Britanniahütte, AD, 2021\n  Allalinhorn 4027m, PD+, 2021\n  Hausstock 3158m, S ridge, 38° / D, 2021\n  Stucklistock 3313m, S ridge, 40° / D, 2021\n  Pigne d’Arolla 3787m, PD+, group leader, 2021\n  Pointes d’Oren 3520m, PD, 2021\n  Mont Brulé 3576m, AD-, 2021\n  Graubünden Haute Route, 7 days, group leader, 2021 - video 📺\n  Piz d’Agnel 3204m, PD\n  Pischahorn 2980m, PD\n  Egghorn 3127m, PD\n  Buinlücke, 45°\n    Piz Fess 2881m, PD, group leader, 2021\n  Piz Tomül 2946m, PD, group leader, 2021\n  Wissigstock 2887m, PD, group leader 2019\n  Piz Calderas 3397m, AD+, 2019\n  Piz Linard 3410m, AD, 2019\n  Piz d’Err 3377m, AD, 2019\n  Pizzo Centrale 2999m, from Gemsstock / S ridge, AD-, group leader, 2019\n  Hengelhorn AD, 2019\n  Spitzmielen 2501m, PD+, group leader, 2019\n  Rossstock 2460m, PD, group leader, 2019\n  Rütistein 2025m, PD, 2019\n  Roggenstock 1776m, by night, 2019\n  Alpinism   Aiguilles Dorées 3519m, traverse E-W, D-, 2024\n  Nadelgrat (Dirruhorn 4035m, Hohberghorn 4219m, Stecknadelhorn 4241m, Nadelhorn 4327m), AD, 2024\n  Dent Blanche 4358m, arrête S, AD, 2024\n  Breithorn 4139m - 4159m - 4164m, traverse E-W, AD+, 2024\n  Lyskamm 4527m - 4479m, traverse E-W, AD, 2024\n  Monte Rosa - Dufourspitze 4634m, traverse W-SE, AD+, 2024\n  Weissmies 4017m, Nordgrat, AD+, 2024\n  Gwächtenhorn 3404m, W-E traverse, PD, 2024\n  Nadelhorn 4327m, PD, 2022\n  Tete Blanche 3421m, North Face, Petite Fourche 3512m, 2022\n  Vrenelisgärtli 2905m, Normal route from Glarnischhütte, PD, group leader, 2021\n  Spaghetti tour, group leader, 2021 - video 📺\n  Castor 4228m, PD I\n  Naso del Liskamm 4272m, PD+\n  Parrotspitze 4434m, PD\n  Zumsteinspitze 4563m, PD\n    Alphubel 4206m, from Täschütte / SE ridge, PD, 2020\n  Mönch 4017m, Normal route, AD, 2020\n  Piz Palü 3882m, traverse W-E from Rifugi dals Chamuotschs-Fortezza, PD 2c, 2020\n  Basodino 3273m, E ridge, 2020\n  Vorder Tierberg 3090, F, 2019\n  Mount Ararat 5137m, Turkey, 2008\n  Alpine climbing   Schmalstöckli 2012m, “Bird on a wire”, 5 pitches, 6b, 2024\n  Läged Windgällen 2572m, …","date":1647379976,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685059200,"objectID":"f7365243bce587e8914663b72130d06b","permalink":"https://vboussange.github.io/pages/alpine_cv/","publishdate":"2022-03-15T22:32:56+01:00","relpermalink":"/pages/alpine_cv/","section":"pages","summary":"Sailing  Bought Tippelei, a 40 year old, 36 feet sailing boat, and sailed it to Norway from May to September 2023 during a sabbatical.   Ski mountaineering / Backcountry skiing   Wintertürmli 3002m, from Meiental, AD-, Switzerland, January 2025","tags":[],"title":"Alpine CV","type":"pages"},{"authors":null,"categories":null,"content":"As biodiversity declines and anthropogenic pressure increases, there is a crucial need for accurate ecosystem models that can capture the dynamics of real ecosystems in order to predict and mitigate collapses. Yet it remains a daunting task to obtain an agreement between current ecosystem models and real world ecosystems. In this project we aim at bridging machine learning techniques and mechanistic models in order to extrapolate beyond data. By embedding prior scientific knowledge in the structure of mechanistic models, this approach should generalise better, be more interpretable and require less data than current techniques.\n","date":1624752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624752000,"objectID":"1eff1a22d5404777b4c035a2d51cf1cb","permalink":"https://vboussange.github.io/project/ecology-informed-ml/","publishdate":"2021-06-27T00:00:00Z","relpermalink":"/project/ecology-informed-ml/","section":"project","summary":"In order to anticipate the responses of ecosystems to anthropogenic pressure and climate change, models that can extrapolate ecological dynamics beyond observations are required. While we know very well the laws of physics, we are very far from a predictive theory of life. Machine learning can help filling this huge gap! 🧠 🤖 🌱","tags":["Scientific machine learning","Ecology and evolution"],"title":"Ecology informed machine learning","type":"project"},{"authors":null,"categories":null,"content":"The fields of Evolutionary Biology and Economics have mutually exchanged ideas that lead to breakthrough for the past two centuries, starting with Darwin who got inspired from the economist Malthus for developing his theory on Evolution. Indeed, it is more and more acknowledged that companies, which are structured around market niches within which they develop competitive and mutualistic interactions, interact in similar ways as biological species. I rely on scientific machine learning to better understand how evolutionary processes arising in economic systems connect to those observed in biological systems.\n","date":1624752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624752000,"objectID":"362f504d06a266f6213f0d42733355dc","permalink":"https://vboussange.github.io/project/econobiology/","publishdate":"2021-06-27T00:00:00Z","relpermalink":"/project/econobiology/","section":"project","summary":"Economic actors develop competitive and mutualistic interactions and evolve through time, in similar ways as groups of species. I rely on scientific machine learning to understand how **evolutionary processes arising in economic systems connect to those observed in biological systems**.","tags":["Economics"],"title":"Econobiology: What can economics learn from biology?","type":"project"},{"authors":["Victor Boussange"],"categories":["Demo",""],"content":"One of the challenges modellers face in biological sciences is to calibrate models in order to match as closely as possible observations and gain predictive power. This can be done via direct measurements through experimental design, but this process is often costly, time consuming and even sometimes not possible. Scientific machine learning addresses this problem by applying optimisation techniques originally developed within the field of machine learning to mechanistic models, allowing to infer parameters directly from observation data. In this blog post, I shall explain the basics of this approach, and how the Julia ecosystem has efficiently embedded such techniques into ready to use packages. This promises exciting perspectives for modellers in all areas of environmental sciences.\n 🚧 This is Work in progress 🚧\n Dynamical systems are models that allow to reproduce, understand and forecast systems. They connect the time variation of the state of the system to the fundamental processes we believe driving it, that is\n$$ \\begin{equation*} \\text{ time variation of } 🌍_t = \\sum \\text{processes acting on } 🌍_t \\end{equation*} $$\nwhere $🌍_t$ denotes the state of the system at time $t$. This translates mathematically into\n$$ \\begin{equation} \\partial_t(🌍_t) = f_\\theta( 🌍_t ) \\end{equation}\\tag{1} $$\nwhere the function $f_\\theta$ captures the ensembles of the processes considered, and depend on the parameters $\\theta$.\nEq. (1) is a Differential Equation, that can be integrated with respect to time to obtain the state of the system at time $t$ given an initial state $🌍_{t_0}$.\n$$ \\begin{equation} 🌍_t = 🌍_{t_0} + \\int_0^t f_\\theta( 🌍_s ) ds \\end{equation}\\tag{2} $$\nDynamical systems have been used for hundreds of years and have successfully captured e.g. the motion of planets (second law of Kepler), the voltage in an electrical circuit, population dynamics (Lotka Volterra equations) and morphogenesis (Turing patterns)…\nSuch models can be used to forecast the state of the system in the future, or can be used in the sense of virtual laboratories. In both cases, one of the requirement is that they reproduce patterns - at least at a qualitative level. To do so, the modeler needs to find the true parameter combination $\\theta$ that correspond to the system under consideration. And this is tricky! In this post we adress this challenge.\nModel calibration  How to determine $\\theta$ so that $\\text{simulations} \\approx \\text{empirical data}$?\n The best way to do that is to design an experiment!\n\nWhen possible, measuring directly the parameters in a controlled experiment with e.g. physical devices is a great approach. This is a very powerful scientific method, used e.g. in global circulation models where scientists can measure the water viscosity, the change in water density with respect to temperature, etc… Unfortunately, such direct methods are often not possible considering other systems.\nAn opposite approach, known as inverse modelling, is to infer the parameters undirectly with the empirical data available.\nParameter exploration One way to find right parameters is to perform parameter exploration, that is, slicing the parameter space and running the model for all parameter combinations chosen. Comparing the simulation results to the empirical data available, one can elect the combination with the higher explanatory power.\nBut as the parameter space becomes larger (higher number of parameters) this becomes tricky. Such problem is often refered to as the curse of dimensionality. Feels very much like being lost in a giant maze. We need more clever technique to get out!\nA Machine Learning problem In machine learning, people try to predict a variable $y$ from predictors $x$ by finding suitable parameters $\\theta$ of a parametric function $F_\\theta$ so that\n$$ \\begin{equation} y = F_\\theta(x) \\end{equation}\\tag{3} $$\nFor example, in computer vision, this function might be designed for the specific task of labelling images, such as for instance\n$F_\\theta ($   $) \\to \\{\\text{cat}, \\text{dog}\\}$\nUsually people use neural networks so that $F_\\theta \\equiv NN_\\theta$, as they are good approximators for high dimensional function (see the Universal approximation theorem). One should really see neural networks as functions ! For example, feed forward neural networks are mathematically described by a series of matrix multiplications and nonlinear operations, i.e. $NN_\\theta (x) = \\sigma_1 \\circ f_1 \\circ \\dots \\circ \\sigma_n \\circ f_n(x)$ where $\\sigma_i$ is an activation function and $f_i$ is linear function $$ \\begin{equation*} f_i (x) = A_i x + b_i . \\end{equation*} $$ Notice that Eq. (2) is similar to Eq. (3)! Indeed one can think of $🌍_0$ as the analogous to $x$ - i.e. the predictor - and $🌍_t$ as the variable $y$ to predict:\n$$ \\begin{equation*} 🌍_t = F_\\theta(🌍_{t_0}) \\end{equation*} $$\nwhere $$F_\\theta (🌍_{t_0}) \\equiv 🌍_{t_0} + \\int_0^t f_\\theta( 🌍_s ) ds .$$\nWith this perspective in mind, techniques developed within the field …","date":1610150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610150400,"objectID":"797a7c7bc12dd6a48fbdcabd69bbaaa2","permalink":"https://vboussange.github.io/post/param-inference/","publishdate":"2021-01-09T00:00:00Z","relpermalink":"/post/param-inference/","section":"post","summary":"One of the challenges modellers face in biological sciences is to calibrate models in order to match as closely as possible observations and gain predictive power. Scientific machine learning addresses this problem by applying optimisation techniques originally developed within the field of machine learning to mechanistic models, allowing  to infer parameters directly from observation data.","tags":["Scientific Machine Learning","Julia"],"title":"Parameter Inference in dynamical systems","type":"post"},{"authors":null,"categories":null,"content":"Partial Differential Equations (PDEs) are equations that arise in a variety of models in physics, engineering, finance and biology. I develop numerical methods based on machine learning techniques to simulate a special class of PDEs in high dimension, that can capture non-locality (cf generic form of the PDEs below). Such PDEs permit to e.g. model the evolution of biological populations in a realistic manner, by describing the dynamics of several traits characterising individuals. For plants, traits correspond for instance to flower colour, specific leaf area, seed mass, etc…. These characteristics define a high dimensional space that must be considered when modelling ecological and evolutionary processes in biological populations, as they determine the overall fitness of a population in a given environment.\n   Trait diversity in the genus Hemerocallis. Plants can be caracterised by many different traits, all of which can be assigned numerical values: Flower colour, specific leaf area, seed mass, Plant nitrogen fixation capacity, Leaf shape, Flower sex, plant woodiness. Source: H Cui et al. 2019  High dimensionality leads to numerical difficulties in simulating the models. The methods I develop overcome this so-called “curse of dimensionality”.\nThe equation below defines the class of PDEs I am interested in. These PDEs are also referred in the literature as non-local reaction diffusion equations.\n$$ \\begin{aligned} (\\tfrac{\\partial}{\\partial t}u)(t,x) \u0026amp;= \\int_{D} f\\big(t,x,{\\bf x}, u(t,x),u(t,{\\bf x}), ( \\nabla_x u )(t,x ),( \\nabla_x u )(t,{\\bf x} ) \\big) \\, \\nu_x(d{\\bf x}) \\\\ \u0026amp; \\quad + \\big\\langle \\mu(t,x), ( \\nabla_x u )( t,x ) \\big\\rangle + \\tfrac{ 1 }{ 2 } \\text{Trace}\\big( \\sigma(t,x) [ \\sigma(t,x) ]^* ( \\text{Hess}_x u)( t,x ) \\big). \\end{aligned} $$\nI have implemented those schemes in HighDimPDE.jl, a Julia package that should allow scientists to develop models that better capture the complexity of life. These techniques extend beyond biology and are also relevant for other fields, such as finance.\n","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"6ffa67f004815d29d42398ae209b39b4","permalink":"https://vboussange.github.io/project/highdimpde/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/project/highdimpde/","section":"project","summary":"[Partial Differential Equations](https://en.wikipedia.org/wiki/Partial_differential_equation) (PDEs) are equations that arise in a variety of models in physics, engineering, finance and biology. I develop **numerical schemes** based on **machine learning techniques** to solve for a special class of PDEs (cf below) in high dimension. ","tags":["Deep Learning","PDEs","Applied Mathematics","Scientific machine learning"],"title":"Machine learning to solve highly dimensional non-local nonlinear PDEs","type":"project"},{"authors":null,"categories":null,"content":"Biodiversity results from differentiation mechanisms developing within biological populations. Such mechanisms are influenced by the properties of the landscape over which individuals interact, disperse and evolve. The documentation of high levels of species diversity in complex mountain region or riverine systems suggest that some peculiar landscape properties foster diversity. I use mathematical models to investigate how connectivity and habitat heterogeneity foster diversity. My modeling approach complements empirical work on biodiversity by connecting documented patterns to the processes that have generated it.\n","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"4c1b955ef859e40f8b2c6a3d57f0f01b","permalink":"https://vboussange.github.io/project/diff-in-graphs/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/project/diff-in-graphs/","section":"project","summary":"Biodiversity results from differentiation mechanisms developing within biological populations. Such mechanisms are influenced by the properties of the landscape over which individuals interact, disperse and evolve. The documentation of high levels of species diversity in complex mountain region or riverine systems suggest that some **peculiar landscape properties foster diversity**. I use mathematical models to investigate how landscape connectivity and habitat heterogeneity foster diversity.","tags":["Biodiversity","Population dynamics","Mathematical modelling","Ecology and evolution"],"title":"Gaining a mechanistic understanding of spatial biodiversity patterns","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://vboussange.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]